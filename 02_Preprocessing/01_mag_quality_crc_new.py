# -*- coding: utf-8 -*-
"""MAG_Quality_CRC_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JNNqfE9LykEntQr5tCXZHYVn1lgrGAfU
"""

# ============================
#   Merge Derep Mags quality with respective quality from raw MAG quality
# ============================

import pandas as pd


# ======================================
#   2. PATHS TO YOUR FILES IN DRIVE
# ======================================
derep_path = "/content/derep_genomeInformation.csv"
checkm_path = "/content/all_checkm2_non_derep_combined.csv"
output_path = "/content/derep_with_checkm2_merged.csv"


# =================================================================
#   3. LOAD DEREP FILE (this file contains the correct Group values)
# =================================================================
derep = pd.read_csv(derep_path)

# Clean column names
derep.columns = derep.columns.str.strip()

# Force dtype to string for stable joins
derep["Sample_ID"] = derep["Sample_ID"].astype(str)
derep["Bins"] = derep["Bins"].astype(str)


# =================================================================
#   4. LOAD AND FIX CHECKM2 CSV (NO SKIPPING, FIX MALFORMED LINES)
# =================================================================
def fix_malformed_csv(path):
    with open(path, "r") as f:
        lines = f.readlines()

    header = lines[0].strip().split(",")
    n = len(header)

    rows = []
    for line in lines[1:]:
        parts = line.rstrip("\n").split(",")

        if len(parts) == n:
            rows.append(parts)

        elif len(parts) < n:
            rows.append(parts + [""] * (n - len(parts)))

        else:
            fixed = parts[:n-1]
            merged_last = ",".join(parts[n-1:])
            fixed.append(merged_last)
            rows.append(fixed)

    return pd.DataFrame(rows, columns=header)

checkm = fix_malformed_csv(checkm_path)


# =================================================================
#   5. CREATE Name IN BOTH FILES FOR ALIGNMENT
# =================================================================
derep["Name"] = derep["Sample_ID"] + "_" + derep["Bins"]

if "Name" not in checkm.columns:
    # if checkm Name column is differently named, find it:
    # this is robust and finds the right column even if misnamed
    for col in checkm.columns:
        if "name" in col.lower():
            checkm = checkm.rename(columns={col: "Name"})
            break


# =================================================================
#   6. MERGE derep + checkm
# =================================================================
merged = derep.merge(checkm, on="Name", how="left")


# =================================================================
#   7. GUARANTEED FIX:
#      FORCE RE-INSERT GROUP FROM DEREP BY MAPPING
# =================================================================
group_map = dict(zip(derep["Name"], derep["Group"]))
merged["Group"] = merged["Name"].map(group_map)


# =================================================================
#   8. ORDER COLUMNS EXACTLY AS REQUIRED
# =================================================================
final_cols = [
    "Group", "Sample_ID", "Bins", "length", "N50", "centrality",
    "Completeness", "Contamination", "Completeness_Model_Used",
    "Translation_Table_Used", "Coding_Density", "Contig_N50",
    "Average_Gene_Length", "Genome_Size", "GC_Content",
    "Total_Coding_Sequences", "Total_Contigs", "Max_Contig_Length"
]

existing_cols = [c for c in final_cols if c in merged.columns]

final_df = merged[existing_cols]


# =================================================================
#   9. SAVE OUTPUT FILE
# =================================================================
final_df.to_csv(output_path, index=False)
print("DONE — Group column FORCED from derep and saved to:")
print(output_path)

#Properly load and format input file
import pandas as pd

def fix_malformed_csv(path):
    with open(path, "r") as f:
        lines = f.readlines()

    header = lines[0].rstrip("\n").split(",")
    n_cols = len(header)

    cleaned_rows = []

    for line in lines[1:]:
        parts = line.rstrip("\n").split(",")

        # Exact match
        if len(parts) == n_cols:
            cleaned_rows.append(parts)

        # Too few columns → pad
        elif len(parts) < n_cols:
            cleaned_rows.append(parts + [""] * (n_cols - len(parts)))

        # Too many columns → combine extras into last column
        else:
            fixed = parts[:n_cols - 1]
            merged_last = ",".join(parts[n_cols - 1:])
            fixed.append(merged_last)
            cleaned_rows.append(fixed)

    return pd.DataFrame(cleaned_rows, columns=header)

# Load your corrected combined dataset:
df = fix_malformed_csv("all_checkm2_non_derep_combined.csv")
print(df.head())
print(df.shape)

# ============================
# NATURE-STYLE KDE PLOTS
# ============================

def plot_kde(metric, filename, xlabel):
    if metric not in df.columns:
        print(f"[skip] {metric} missing in df.")
        return

    # Nature-style soft palette (Set2 = publication friendly)
    nat_palette = sns.color_palette("Set2", len(cohorts))
    palette_map = {c: nat_palette[i] for i, c in enumerate(cohorts)}

    sns.set_style("white")
    fig, ax = plt.subplots(figsize=(4, 4))

    plotted_any = False

    for cohort in cohorts:

        sub = df[df["Group"] == cohort][metric].dropna()

        if len(sub) == 0:
            continue

        # If too few points, fallback to rug-like strip
        if len(sub) < 5:
            sns.stripplot(
                x=sub,
                y=[0] * len(sub),
                color=palette_map[cohort],
                size=5,
                alpha=0.4,
                label=cohort,
                ax=ax
            )
            plotted_any = True
            continue

        # VERY LIGHT shading (Nature-style)
        sns.kdeplot(
            sub,
            fill=True,
            alpha=0.02,                # <<< VERY light fill
            color=palette_map[cohort],
            linewidth=0,
            ax=ax
        )

        # Clean outline on top
        sns.kdeplot(
            sub,
            fill=False,
            color=palette_map[cohort],
            linewidth=2.2,
            label=cohort,
            ax=ax
        )

        plotted_any = True

    if not plotted_any:
        print(f"[skip] No valid KDE data for {metric}.")
        return

    ax.set_xlabel(xlabel, fontsize=12)
    ax.set_ylabel("Density", fontsize=12)

    style_axes(ax)

    # Force legend to display
    handles, labels = ax.get_legend_handles_labels()
    ax.legend(
        handles,
        labels,
        title="",
        fontsize=6,
        frameon=False
    )

    plt.tight_layout()
    plt.savefig(filename, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"[OK] KDE SAVED → {filename}")


# ============================
# RUN BOTH KDE PLOTS
# ============================

plot_kde("Completeness",
         "cohort_completeness_kde.png",
         "Completeness (%)")

plot_kde("Contamination",
         "cohort_contamination_kde.png",
         "Contamination (%)")

# ============================
# SAFE SCATTER PLOT FUNCTION
# ============================
def plot_scatter(x, y, xlabel, ylabel, filename):

    # Check column existence
    if not has_cols(df, [x, y]):
        print(f"[skip] missing {x} or {y}")
        return

    # Clean numeric values
    df_clean = df[[x, y, "Group"]].copy()
    df_clean[x] = pd.to_numeric(df_clean[x], errors="coerce")
    df_clean[y] = pd.to_numeric(df_clean[y], errors="coerce")

    # Drop invalid values
    df_clean = df_clean.replace([np.inf, -np.inf], np.nan).dropna()

    if len(df_clean) < 3:
        print(f"[skip] not enough valid points for {x} vs {y}")
        return

    sns.set_style("white")
    fig, ax = plt.subplots(figsize=(5, 5))

    sns.scatterplot(
        data=df_clean,
        x=x, y=y,
        hue="Group",
        hue_order=cohorts,
        palette=palette,
        s=40,
        alpha=0.7,
        linewidth=0.4,
        edgecolor="black",
        ax=ax
    )

    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    style_axes(ax)

    # ===============================
    # REDUCE LEGEND SIZE SAFELY
    # ===============================
    legend = ax.get_legend()
    if legend is not None:
        legend.set_title("")
        legend.set_frame_on(True)

        # Smaller legend text
        for text in legend.get_texts():
            text.set_fontsize(8)

        # Correct handle resizing for both Line2D and PathCollection
        for handle in legend.legend_handles:
            if hasattr(handle, "set_sizes"):      # scatter PathCollection
                handle.set_sizes([30])
            elif hasattr(handle, "set_markersize"):  # Line2D
                handle.set_markersize(6)

    plt.savefig(filename, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"[OK] scatter saved → {filename}")



# ============================
# RUN ALL THREE SCATTER PLOTS
# ============================

plot_scatter(
    "GC_Content",
    "Genome_Size",
    "GC Content (%)",
    "Genome Size (bp)",
    "gc_vs_genome_size_scatter_cohorts.png"
)

plot_scatter(
    "Completeness",
    "Coding_Density",
    "Completeness (%)",
    "Coding Density",
    "completeness_vs_coding_density_cohorts.png"
)

plot_scatter(
    "Average_Gene_Length",
    "Total_Coding_Sequences",
    "Avg Gene Length",
    "Total Coding Sequences",
    "gene_length_vs_coding_sequences_cohorts.png"
)

# ============================
#  CLEANED CORRELATION TABLE
# ============================

tests = [
    ("GC_Content", "Genome_Size"),
    ("Completeness", "Coding_Density"),
    ("Contig_N50", "Max_Contig_Length"),
    ("Average_Gene_Length", "Total_Coding_Sequences")
]

corr_rows = []

for x, y in tests:

    if not has_cols(df, [x, y]):
        corr_rows.append({
            "Comparison": f"{x} vs {y}",
            "Pearson": None,
            "Spearman": None,
            "Valid Points": 0
        })
        continue

    # Clean numerics
    clean = df[[x, y]].copy()
    clean[x] = pd.to_numeric(clean[x], errors="coerce")
    clean[y] = pd.to_numeric(clean[y], errors="coerce")

    # Remove bad rows
    clean = clean.replace([np.inf, -np.inf], np.nan).dropna()

    n = len(clean)

    if n < 3:
        corr_rows.append({
            "Comparison": f"{x} vs {y}",
            "Pearson": None,
            "Spearman": None,
            "Valid Points": n
        })
        continue

    pearson_corr, _ = pearsonr(clean[x], clean[y])
    spearman_corr, _ = spearmanr(clean[x], clean[y])

    corr_rows.append({
        "Comparison": f"{x} vs {y}",
        "Pearson": round(pearson_corr, 3),
        "Spearman": round(spearman_corr, 3),
        "Valid Points": n
    })

corr_df = pd.DataFrame(corr_rows)
corr_df.to_csv("correlation_results_cohorts.csv", index=False)

print("\n===== CORRELATION TABLE GENERATED =====")
print(corr_df)
print("Saved → correlation_results_cohorts.csv")



# ============================
#  STACKED BARPLOT OF QUALITY
# ============================

# Ensure all quality classes exist
tier_order = ["High-quality", "Medium-quality", "Low-quality"]

table = (
    df.groupby(["Group", "Quality_Tier"])
      .size()
      .unstack(fill_value=0)
      .reindex(cohorts)               # ensures cohort order
      .reindex(columns=tier_order, fill_value=0)
      .astype(int)
)

print("\n===== MAG QUALITY SUMMARY =====")
print(table)
table.to_csv("MAG_quality_summary_by_cohort.csv")


# Plot
fig, ax = plt.subplots(figsize=(5, 5))

table.plot(
    kind="bar",
    stacked=True,
    edgecolor="black",
    width=0.75,
    color=['#1b9e77', '#d95f02', '#7570b3'],
    ax=ax
)

# --- REDUCE LEGEND SIZE HERE ---
ax.legend(
    fontsize=10,   # <<< smaller legend text
    title="",     # no title
    frameon=True # cleaner for publication
)

ax.set_ylabel("Number of MAGs")
ax.set_xlabel("")
plt.xticks(rotation=45, ha="right")

style_axes(ax)

plt.tight_layout()
plt.savefig("quality_tier_stacked_bar_cohorts.png", dpi=600)
plt.close()

print("Saved → quality_tier_stacked_bar_cohorts.png")

correlation_tests = [
    ("Contig_N50", "Completeness"),
    ("Contig_N50", "Contamination"),
    ("Contig_N50", "Genome_Size")
]

rows = []

for cohort in cohorts:
    df_sub = df[df["Group"] == cohort].copy()

    for x, y in correlation_tests:

        if not has_cols(df_sub, [x, y]):
            rows.append({"Cohort": cohort, "Comparison": f"{x} vs {y}",
                         "Pearson": None, "Spearman": None})
            continue

        df_sub[x] = pd.to_numeric(df_sub[x], errors="coerce")
        df_sub[y] = pd.to_numeric(df_sub[y], errors="coerce")

        clean = df_sub[[x, y]].replace([np.inf, -np.inf], np.nan).dropna()

        if len(clean) < 3:
            rows.append({"Cohort": cohort, "Comparison": f"{x} vs {y}",
                         "Pearson": None, "Spearman": None})
            continue

        p, _ = pearsonr(clean[x], clean[y])
        s, _ = spearmanr(clean[x], clean[y])

        rows.append({
            "Cohort": cohort,
            "Comparison": f"{x} vs {y}",
            "Pearson": round(p, 3),
            "Spearman": round(s, 3)
        })

cohort_corr = pd.DataFrame(rows)
cohort_corr.to_csv("per_cohort_n50_correlations.csv", index=False)
print("Saved → per_cohort_n50_correlations.csv")

"""Start analysis of DEREPLICATED MAGS

"""

import pandas as pd

def fix_malformed_csv(path):
    with open(path, "r") as f:
        lines = f.readlines()

    header = lines[0].rstrip("\n").split(",")
    n_cols = len(header)

    cleaned_rows = []

    for line in lines[1:]:
        parts = line.rstrip("\n").split(",")

        # Exact match
        if len(parts) == n_cols:
            cleaned_rows.append(parts)

        # Too few columns → pad
        elif len(parts) < n_cols:
            cleaned_rows.append(parts + [""] * (n_cols - len(parts)))

        # Too many columns → combine extras into last column
        else:
            fixed = parts[:n_cols - 1]
            merged_last = ",".join(parts[n_cols - 1:])
            fixed.append(merged_last)
            cleaned_rows.append(fixed)

    return pd.DataFrame(cleaned_rows, columns=header)

# Load your corrected combined dataset:
df = fix_malformed_csv("derep_with_checkm2_merged.csv")
print(df.head())
print(df.shape)

# ============================
# NATURE-STYLE KDE PLOTS
# ============================

def plot_kde(metric, filename, xlabel):
    if metric not in df.columns:
        print(f"[skip] {metric} missing in df.")
        return

    # Nature-style soft palette
    nat_palette = sns.color_palette("Set2", len(cohorts))
    palette_map = {c: nat_palette[i] for i, c in enumerate(cohorts)}

    sns.set_style("white")
    fig, ax = plt.subplots(figsize=(4, 4))

    plotted_any = False

    for cohort in cohorts:

        # Convert column to numeric
        sub = pd.to_numeric(df[df["Group"] == cohort][metric], errors="coerce").dropna()

        if len(sub) == 0:
            continue

        # Rug fallback if < 5 points
        if len(sub) < 5:
            sns.stripplot(
                x=sub,
                y=[0] * len(sub),
                color=palette_map[cohort],
                size=5,
                alpha=0.35,
                label=cohort,
                ax=ax
            )
            plotted_any = True
            continue

        # --- VERY LIGHT SHADED KDE ---
        sns.kdeplot(
            x=sub,
            fill=True,
            alpha=0.02,
            color=palette_map[cohort],
            linewidth=0,
            ax=ax
        )

        # --- CLEAN OUTLINE KDE ---
        sns.kdeplot(
            x=sub,
            fill=False,
            color=palette_map[cohort],
            linewidth=2.2,
            label=cohort,
            ax=ax
        )

        plotted_any = True

    if not plotted_any:
        print(f"[skip] No valid KDE data for {metric}.")
        return

    ax.set_xlabel(xlabel, fontsize=12)
    ax.set_ylabel("Density", fontsize=12)

    style_axes(ax)

    handles, labels = ax.get_legend_handles_labels()
    ax.legend(
        handles,
        labels,
        title="",
        fontsize=6,
        frameon=False
    )

    plt.tight_layout()
    plt.savefig(filename, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"[OK] KDE SAVED → {filename}")

# ============================
# RUN BOTH KDE PLOTS
# ============================

plot_kde("Completeness",
         "cohort_completeness_kde.png",
         "Completeness (%)")

plot_kde("Contamination",
         "cohort_contamination_kde.png",
         "Contamination (%)")

# ============================
# SAFE SCATTER PLOT FUNCTION
# ============================
def plot_scatter(x, y, xlabel, ylabel, filename):

    # Check column existence
    if not has_cols(df, [x, y]):
        print(f"[skip] missing {x} or {y}")
        return

    # Clean numeric values
    df_clean = df[[x, y, "Group"]].copy()
    df_clean[x] = pd.to_numeric(df_clean[x], errors="coerce")
    df_clean[y] = pd.to_numeric(df_clean[y], errors="coerce")

    # Drop invalid values
    df_clean = df_clean.replace([np.inf, -np.inf], np.nan).dropna()

    if len(df_clean) < 3:
        print(f"[skip] not enough valid points for {x} vs {y}")
        return

    sns.set_style("white")
    fig, ax = plt.subplots(figsize=(5, 5))

    sns.scatterplot(
        data=df_clean,
        x=x, y=y,
        hue="Group",
        hue_order=cohorts,
        palette=palette,
        s=40,
        alpha=0.7,
        linewidth=0.4,
        edgecolor="black",
        ax=ax
    )

    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    style_axes(ax)

    # ===============================
    # REDUCE LEGEND SIZE SAFELY
    # ===============================
    legend = ax.get_legend()
    if legend is not None:
        legend.set_title("")
        legend.set_frame_on(True)

        # Smaller legend text
        for text in legend.get_texts():
            text.set_fontsize(8)

        # Correct handle resizing for both Line2D and PathCollection
        for handle in legend.legend_handles:
            if hasattr(handle, "set_sizes"):      # scatter PathCollection
                handle.set_sizes([30])
            elif hasattr(handle, "set_markersize"):  # Line2D
                handle.set_markersize(6)

    plt.savefig(filename, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"[OK] scatter saved → {filename}")



# ============================
# RUN ALL THREE SCATTER PLOTS
# ============================

plot_scatter(
    "GC_Content",
    "Genome_Size",
    "GC Content (%)",
    "Genome Size (bp)",
    "gc_vs_genome_size_scatter_cohorts.png"
)

plot_scatter(
    "Completeness",
    "Coding_Density",
    "Completeness (%)",
    "Coding Density",
    "completeness_vs_coding_density_cohorts.png"
)

plot_scatter(
    "Average_Gene_Length",
    "Total_Coding_Sequences",
    "Avg Gene Length",
    "Total Coding Sequences",
    "gene_length_vs_coding_sequences_cohorts.png"
)

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# ----------------------------------------------------------
#  REQUIREMENTS: assumes df, cohorts, palette, has_cols, style_axes exist
# ----------------------------------------------------------

# ============================
#  Ensure numeric columns BEFORE any analysis
# ============================
df["Completeness"] = pd.to_numeric(df["Completeness"], errors="coerce")
df["Contamination"] = pd.to_numeric(df["Contamination"], errors="coerce")

# Optional: drop rows missing BOTH values
df = df.dropna(subset=["Completeness", "Contamination"], how="all")

# ============================
#  1. CLEANED CORRELATION TABLE
# ============================

tests = [
    ("GC_Content", "Genome_Size"),
    ("Completeness", "Coding_Density"),
    ("Contig_N50", "Max_Contig_Length"),
    ("Average_Gene_Length", "Total_Coding_Sequences")
]

corr_rows = []

for x, y in tests:

    if not has_cols(df, [x, y]):
        corr_rows.append({
            "Comparison": f"{x} vs {y}",
            "Pearson": None,
            "Spearman": None,
            "Valid Points": 0
        })
        continue

    clean = df[[x, y]].copy()
    clean[x] = pd.to_numeric(clean[x], errors="coerce")
    clean[y] = pd.to_numeric(clean[y], errors="coerce")

    clean = clean.replace([np.inf, -np.inf], np.nan).dropna()
    n = len(clean)

    if n < 3:
        corr_rows.append({
            "Comparison": f"{x} vs {y}",
            "Pearson": None,
            "Spearman": None,
            "Valid Points": n
        })
        continue

    pearson_corr, _ = pearsonr(clean[x], clean[y])
    spearman_corr, _ = spearmanr(clean[x], clean[y])

    corr_rows.append({
        "Comparison": f"{x} vs {y}",
        "Pearson": round(pearson_corr, 3),
        "Spearman": round(spearman_corr, 3),
        "Valid Points": n
    })

corr_df = pd.DataFrame(corr_rows)
corr_df.to_csv("correlation_results_cohorts.csv", index=False)

print("\n===== CORRELATION TABLE GENERATED =====")
print(corr_df)
print("Saved → correlation_results_cohorts.csv")


# ============================
#  2. STACKED BARPLOT OF QUALITY
# ============================

# ---- FIX: FORCE REBUILD QUALITY_TIER WITH NUMERIC SAFE CHECK ----
def classify_quality(r):
    comp = r["Completeness"]
    cont = r["Contamination"]

    # If either numeric is missing, classify conservatively
    if pd.isna(comp) or pd.isna(cont):
        return "Low-quality"

    if comp >= 90 and cont <= 5:
        return "High-quality"
    elif comp >= 50 and cont <= 10:
        return "Medium-quality"
    else:
        return "Low-quality"

df["Quality_Tier"] = df.apply(classify_quality, axis=1)

tier_order = ["High-quality", "Medium-quality", "Low-quality"]

table = (
    df.groupby(["Group", "Quality_Tier"])
      .size()
      .unstack(fill_value=0)
      .reindex(cohorts)
      .reindex(columns=tier_order, fill_value=0)
      .astype(int)
)

print("\n===== MAG QUALITY SUMMARY =====")
print(table)
table.to_csv("MAG_quality_summary_by_cohort.csv")

# ---- PLOT ----
fig, ax = plt.subplots(figsize=(5, 5))

table.plot(
    kind="bar",
    stacked=True,
    edgecolor="black",
    width=0.75,
    color=['#1b9e77', '#d95f02', '#7570b3'],
    ax=ax
)

ax.legend(
    fontsize=8,
    title="",
    frameon=True
)

ax.set_ylabel("Number of MAGs")
ax.set_xlabel("")
plt.xticks(rotation=45, ha="right")

style_axes(ax)

plt.tight_layout()
plt.savefig("quality_tier_stacked_bar_cohorts.png", dpi=600)
plt.close()

print("Saved → quality_tier_stacked_bar_cohorts.png")
print("Saved → MAG_quality_summary_by_cohort.csv")
print("\nALL ANALYSES COMPLETE ✔")





correlation_tests = [
    ("Contig_N50", "Completeness"),
    ("Contig_N50", "Contamination"),
    ("Contig_N50", "Genome_Size")
]

rows = []

for cohort in cohorts:
    df_sub = df[df["Group"] == cohort].copy()

    for x, y in correlation_tests:

        if not has_cols(df_sub, [x, y]):
            rows.append({"Cohort": cohort, "Comparison": f"{x} vs {y}",
                         "Pearson": None, "Spearman": None})
            continue

        df_sub[x] = pd.to_numeric(df_sub[x], errors="coerce")
        df_sub[y] = pd.to_numeric(df_sub[y], errors="coerce")

        clean = df_sub[[x, y]].replace([np.inf, -np.inf], np.nan).dropna()

        if len(clean) < 3:
            rows.append({"Cohort": cohort, "Comparison": f"{x} vs {y}",
                         "Pearson": None, "Spearman": None})
            continue

        p, _ = pearsonr(clean[x], clean[y])
        s, _ = spearmanr(clean[x], clean[y])

        rows.append({
            "Cohort": cohort,
            "Comparison": f"{x} vs {y}",
            "Pearson": round(p, 3),
            "Spearman": round(s, 3)
        })

cohort_corr = pd.DataFrame(rows)
cohort_corr.to_csv("per_cohort_n50_correlations.csv", index=False)
print("Saved → per_cohort_n50_correlations.csv")
