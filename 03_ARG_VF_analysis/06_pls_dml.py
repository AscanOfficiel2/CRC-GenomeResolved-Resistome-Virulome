# -*- coding: utf-8 -*-
"""PLS-DML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19aGQTMZ6hmGxaQXzD9xSXFyjKpV35pd2
"""

!pip install econml

# ============================================================
# Cross-Validated PLS (stable) + Cross-Fitted DML + Bootstrapping
# ============================================================

import os
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from econml.dml import LinearDML
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from statsmodels.stats.multitest import multipletests
from scipy.stats import norm
import warnings

warnings.filterwarnings("ignore")

# ------------------------------------------------------------
#  Reproducibility
# ------------------------------------------------------------
SEED = 42
np.random.seed(SEED)

# ------------------------------------------------------------
# 1) Load & align data
# ------------------------------------------------------------
tpm = pd.read_csv("/content/ARG_VF_TPM_clr_batch_corrected_v2.csv", index_col=0)           # features × samples
meta = pd.read_csv("/content/ARG_VF_metadata_final_matched.csv") # has Sample_ID, Group, Age, BMI, Sex
tpm = tpm.T
common = tpm.columns.intersection(meta["Sample_ID"])
tpm = tpm[common]
meta = meta[meta["Sample_ID"].isin(common)].reset_index(drop=True)
tpm = tpm[meta["Sample_ID"]]  # enforce order

# Encode outcome (ordinal: Healthy=0, Adenoma=1, Cancer=2)
le = LabelEncoder()
meta["Y"] = le.fit_transform(meta["Group"])

# Confounders
meta["Sex"] = meta["Sex"].astype(str).str.lower().map({"male": 1, "m": 1, "female": 0, "f": 0})
Xconf = (
    meta[["Age", "BMI", "Sex"]]
    .apply(pd.to_numeric, errors="coerce")
    .fillna(meta[["Age", "BMI", "Sex"]].median())
    .values
)
Y = meta["Y"].values

# Feature matrix (samples × features)
X_raw = tpm.T.values
feature_names = tpm.index.tolist()
n_samples, n_features = X_raw.shape
print(f" Data aligned — Samples: {n_samples} | Features: {n_features}")

# ------------------------------------------------------------
# 2) Stable PLS via cross-validated loadings (sign-aligned)
# ------------------------------------------------------------
n_components = 10
n_splits = 5
cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)

# Standardize features once (global scaler for stability)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)

# Collect x_weights_ across folds, align signs, then average
weights_list = []
ref_weights = None

for fold_idx, (tr, te) in enumerate(cv.split(X_scaled, Y), 1):
    pls = PLSRegression(n_components=n_components)
    pls.fit(X_scaled[tr], Y[tr])
    W = pls.x_weights_.copy()  # shape: (features, n_components)

    if ref_weights is None:
        ref_weights = np.sign(W.sum(axis=0, keepdims=True))  # reference sign per component
        ref_weights[ref_weights == 0] = 1.0

    # Align signs component-wise with reference
    signs = np.sign((W * ref_weights).sum(axis=0, keepdims=True))
    signs[signs == 0] = 1.0
    W_aligned = W * signs
    weights_list.append(W_aligned)

# Average aligned weights
W_avg = np.mean(np.stack(weights_list, axis=2), axis=2)  # (features, n_components)

# Orthonormalize columns (optional but helpful)
# Normalize each component vector to unit norm
W_norm = W_avg / (np.linalg.norm(W_avg, axis=0, keepdims=True) + 1e-12)

# Stable PLS component scores for all samples:
# T = X_scaled @ W_norm
T_pls = X_scaled @ W_norm  # (samples × n_components)

# Save stable loadings and scores
loadings_df = pd.DataFrame(W_norm, index=feature_names, columns=[f"PLS_{i+1}" for i in range(n_components)])
loadings_df.to_csv("Stable_PLS_Loadings.csv")
scores_df = pd.DataFrame(T_pls, index=meta["Sample_ID"], columns=[f"PLS_{i+1}" for i in range(n_components)])
scores_df.to_csv("Stable_PLS_Scores.csv")
print(f"Stable PLS built — {n_components} components (saved: Stable_PLS_Loadings.csv, Stable_PLS_Scores.csv)")

# ------------------------------------------------------------
# 3) Tuned base learners for DML
# ------------------------------------------------------------
gb_params = {
    "n_estimators": 400,
    "learning_rate": 0.05,
    "max_depth": 3,
    "subsample": 1,
    "random_state": SEED
}
rf_params = {
    "n_estimators": 300,
    "max_depth": 3,
    "min_samples_split": 2,
    "min_samples_leaf": 2,
    "max_features": "sqrt",
    "random_state": SEED,
    "n_jobs": -1
}

# ------------------------------------------------------------
# 4) Bootstrapped cross-fitted DML over PLS components
# ------------------------------------------------------------
B = 200                     # number of bootstrap replicates
components = [f"PLS_{i+1}" for i in range(n_components)]
checkpoint_every = 25
os.makedirs("dml_boot_partials", exist_ok=True)

boot_records = []
print(f"Bootstrapping DML with cross-fitting (B={B}, components={n_components})...")

for b in tqdm(range(1, B + 1)):
    # Bootstrap indices
    idx = np.random.choice(n_samples, size=n_samples, replace=True)
    Xb, Yb, Tb = Xconf[idx], Y[idx], T_pls[idx, :]

    for k in range(n_components):
        Ti = Tb[:, [k]]  # single-component as treatment

        # Cross-fitted DML
        dml = LinearDML(
            model_y=GradientBoostingRegressor(**gb_params),
            model_t=RandomForestRegressor(**rf_params),
            discrete_outcome=False,      # Y is treated as continuous ordinal
            discrete_treatment=False,
            cv=5,                        # cross-fitting
            random_state=SEED + b
        )
        dml.fit(Yb, Ti, X=Xb)
        eff = dml.effect(Xb)
        ate = float(np.mean(eff))

        boot_records.append({
            "Bootstrap": b,
            "Component": f"PLS_{k+1}",
            "ATE": ate
        })

    # checkpoint
    if b % checkpoint_every == 0:
        pd.DataFrame(boot_records).to_csv(f"dml_boot_partials/DML_boot_partial_up_to_{b}.csv", index=False)

boot_df = pd.DataFrame(boot_records)
boot_df.to_csv("DML_Bootstrap_Effects_ByComponent.csv", index=False)

# ------------------------------------------------------------
# 5) Summaries (mean, sd, CI, z, p, FDR)
# ------------------------------------------------------------
summ = (
    boot_df.groupby("Component")["ATE"]
    .agg(["mean", "std"])
    .rename(columns={"mean": "ATE_mean_boot", "std": "ATE_std_boot"})
    .reset_index()
)

# 95% CI (normal approximation from bootstrap SD)
summ["Boot_CI_Lower"] = summ["ATE_mean_boot"] - 1.96 * summ["ATE_std_boot"]
summ["Boot_CI_Upper"] = summ["ATE_mean_boot"] + 1.96 * summ["ATE_std_boot"]

# z, p
summ["z"] = summ["ATE_mean_boot"] / (summ["ATE_std_boot"] + 1e-12)
#summ["pval"] = 2 * (1 - norm.cdf(np.abs(summ["z"])))

# FDR
#summ["qval_FDR_BH"] = multipletests(summ["pval"].values, method="fdr_bh")[1]

# Stability score
summ["Stability_Score"] = np.abs(summ["ATE_mean_boot"]) / (summ["ATE_std_boot"] + 1e-12)

summ = summ.sort_values("ATE_mean_boot", ascending=True)  # negative (protective) on top
summ.to_csv("DML_PLS_Bootstrap_Summary.csv", index=False)

print("\nBootstrapping complete!")
print("Saved:")
print("  - Stable_PLS_Loadings.csv")
print("  - Stable_PLS_Scores.csv")
print("  - DML_Bootstrap_Effects_ByComponent.csv")
print("  - DML_PLS_Bootstrap_Summary.csv")

print("\nTop summary (by ATE_mean_boot):")
print(summ.head(10))



# ============================================================
# Extract & Rank Top Genes per PLS Component + Plots
#  - Output table: Component, Rank, Gene, Abs_Loading, ATE_mean_boot, E-value (Point)
#  - Plots: Top genes for most stable PROGRESSION & PROTECTIVE components
# ============================================================

import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.cross_decomposition import PLSRegression
from sklearn.preprocessing import StandardScaler

# -----------------------------
# Config
# -----------------------------
N_COMPONENTS = 10
TOP_N_GENES  = 20   # per component for plots/table
SEED = 42

# -----------------------------
# 1) Load data
# -----------------------------
tpm = pd.read_csv("/content/ARG_VF_TPM_clr_batch_corrected_v2.csv", index_col=0)  # features x samples
meta = pd.read_csv("/content/ARG_VF_metadata_final_matched.csv")

# Align
common = tpm.columns.intersection(meta["Sample_ID"])
tpm = tpm[common]
meta = meta[meta["Sample_ID"].isin(common)].reset_index(drop=True)
tpm = tpm[meta["Sample_ID"]]

# -----------------------------
# 2) Load PLS loadings (or compute)
# -----------------------------
if os.path.exists("Stable_PLS_Loadings.csv"):
    loadings = pd.read_csv("Stable_PLS_Loadings.csv", index_col=0)
    # Ensure column names are PLS_1...PLS_N
    loadings.columns = [f"PLS_{i+1}" for i in range(loadings.shape[1])]
    print(f"Loaded PLS loadings from Stable_PLS_Loadings.csv ({loadings.shape[0]} genes × {loadings.shape[1]} comps)")
else:
    print("Stable_PLS_Loadings.csv not found — refitting PLS for loadings...")
    # Encode labels (Healthy=0, Adenoma=1, Cancer=2)
    grp = meta["Group"].astype('category').cat.codes.values
    # Scale and fit PLS
    X_scaled = StandardScaler().fit_transform(tpm.T.values)  # samples x genes
    pls = PLSRegression(n_components=N_COMPONENTS)
    pls.fit(X_scaled, grp)
    loadings = pd.DataFrame(
        pls.x_loadings_,
        index=tpm.index,
        columns=[f"PLS_{i+1}" for i in range(N_COMPONENTS)]
    )
    loadings.to_csv("Stable_PLS_Loadings.csv")
    print(f"Saved PLS loadings → Stable_PLS_Loadings.csv")

# -----------------------------
# 3) Load component-level stats (ATE + E-value)
# -----------------------------
if os.path.exists("PLS_DML_Evalue_Sensitivity.csv"):
    comp_stats = pd.read_csv("PLS_DML_Evalue_Sensitivity.csv")
    print("Loaded component stats from PLS_DML_Evalue_Sensitivity.csv")
else:
    # Fallback: compute E-values from bootstrap summary if present
    if not os.path.exists("DML_PLS_Bootstrap_Summary.csv"):
        raise FileNotFoundError(
            "Missing PLS_DML_Evalue_Sensitivity.csv and DML_PLS_Bootstrap_Summary.csv. "
            "Please run the DML bootstrap + E-value step first."
        )
    summ = pd.read_csv("DML_PLS_Bootstrap_Summary.csv")
    def compute_evalue(estimate, lower_ci, upper_ci):
        rr = float(np.exp(abs(estimate)))
        rr_ci = float(np.exp(abs(lower_ci if estimate > 0 else upper_ci)))
        def ev(rr_):
            if rr_ <= 1:
                return 1.0
            return rr_ + np.sqrt(rr_ * (rr_ - 1))
        return ev(rr), ev(rr_ci)
    rows = []
    for _, r in summ.iterrows():
        e_point, e_ci = compute_evalue(r["ATE_mean_boot"], r["Boot_CI_Lower"], r["Boot_CI_Upper"])
        rows.append({
            "Component": r["Component"],
            "ATE_mean_boot": r["ATE_mean_boot"],
            "Boot_CI_Lower": r["Boot_CI_Lower"],
            "Boot_CI_Upper": r["Boot_CI_Upper"],
            "E-value (Point)": e_point,
            "E-value (CI)": e_ci,
            "Stability_Score": r.get("Stability_Score", np.nan)
        })
    comp_stats = pd.DataFrame(rows)
    comp_stats.to_csv("PLS_DML_Evalue_Sensitivity.csv", index=False)
    print("Computed E-values → PLS_DML_Evalue_Sensitivity.csv")

# Keep only components that exist in loadings
valid_components = [c for c in comp_stats["Component"] if c in loadings.columns]
comp_stats = comp_stats[comp_stats["Component"].isin(valid_components)].copy()

# -----------------------------
# 4) Rank genes per component by |loading| and merge stats
# -----------------------------
abs_loadings = loadings.abs()

records = []
for comp in valid_components:
    ranked = abs_loadings[comp].sort_values(ascending=False).head(TOP_N_GENES)
    for rank, (gene, abs_w) in enumerate(ranked.items(), start=1):
        records.append({
            "Component": comp,
            "Rank": rank,
            "Gene": gene,
            "Abs_Loading": abs_w
        })

ranked_df = pd.DataFrame(records)

# Merge component-level ATE & E-value into gene-ranked table
ranked_annot = ranked_df.merge(
    comp_stats[["Component", "ATE_mean_boot", "E-value (Point)"]],
    on="Component",
    how="left"
).sort_values(["Component", "Rank"])

ranked_annot.to_csv("PLS_TopGenes_Ranked_with_ComponentStats.csv", index=False)
print(f"Saved → PLS_TopGenes_Ranked_with_ComponentStats.csv ({len(ranked_annot)} rows)")

# -----------------------------
# 5) Pick most stable progression/protective components
#    (stability by Stability_Score, direction by ATE sign)
# -----------------------------
if "Stability_Score" not in comp_stats.columns:
    # compute a proxy if not provided
    if {"ATE_mean_boot", "Boot_CI_Lower", "Boot_CI_Upper"}.issubset(comp_stats.columns):
        comp_stats["ATE_std_boot_proxy"] = (comp_stats["Boot_CI_Upper"] - comp_stats["Boot_CI_Lower"]) / (2*1.96 + 1e-9)
        comp_stats["Stability_Score"] = comp_stats["ATE_mean_boot"].abs() / (comp_stats["ATE_std_boot_proxy"] + 1e-12)
    else:
        comp_stats["Stability_Score"] = comp_stats["ATE_mean_boot"].abs()

# Progression: ATE > 0, highest stability
prog_comp = comp_stats[comp_stats["ATE_mean_boot"] > 0].sort_values("Stability_Score", ascending=False)
# Protective: ATE < 0, highest stability
prot_comp = comp_stats[comp_stats["ATE_mean_boot"] < 0].sort_values("Stability_Score", ascending=False)

selected = []
if len(prog_comp) > 0:
    selected.append(("progression", prog_comp.iloc[0]["Component"]))
if len(prot_comp) > 0:
    selected.append(("protective", prot_comp.iloc[0]["Component"]))

print("Selected components:")
for kind, comp in selected:
    ate = float(comp_stats.loc[comp_stats["Component"] == comp, "ATE_mean_boot"].iloc[0])
    stab = float(comp_stats.loc[comp_stats["Component"] == comp, "Stability_Score"].iloc[0])
    ev   = float(comp_stats.loc[comp_stats["Component"] == comp, "E-value (Point)"].iloc[0])
    print(f"  • {comp} — {kind} | ATE={ate:.3f}, Stability={stab:.3f}, E-value={ev:.2f}")

# -----------------------------
# 6) Plot top genes for selected components
# -----------------------------
sns.set(style="white", context="talk")
plt.rcParams.update({
    "axes.linewidth": 2,
    "xtick.direction": "out",
    "ytick.direction": "out",
})

# -----------------------------
# 6) Plot top genes for selected components — SINGLE COLOR
# -----------------------------
sns.set(style="white", context="talk")
plt.rcParams.update({
    "axes.linewidth": 2,
    "xtick.direction": "out",
    "ytick.direction": "out",
})

for kind, comp in selected:
    sub = ranked_annot[ranked_annot["Component"] == comp] \
            .sort_values("Rank") \
            .head(TOP_N_GENES).copy()

    ATE_val = sub["ATE_mean_boot"].iloc[0]

    # -------------------------------------------------------
    # SINGLE COLOR RULE:
    #     ATE > 0  → BLUE
    #     ATE < 0  → PURPLE
    # -------------------------------------------------------
    if ATE_val > 0:
        bar_color = "#E74C3C"   # blue
        direction = "Progression (↑ in disease)"
    else:
        bar_color = "#2ECC71"   # purple
        direction = "Protective (↓ in disease)"

    plt.figure(figsize=(5, 5))
    ax = sns.barplot(
        data=sub,
        x="Abs_Loading",
        y="Gene",
        color=bar_color,
        edgecolor="black",
        linewidth=1.0
    )

    ax.set_xlabel("Absolute Loading Strength", fontsize=9, weight="bold")
    ax.set_ylabel("")

    title = (
        f"{comp} — {direction}\n"
        f"ATE={ATE_val:.3f},  "
        f"E-value={sub['E-value (Point)'].iloc[0]:.2f}"
    )
    ax.set_title(title, fontsize=11, fontweight="bold", pad=12)

    # Bold gene labels
    ax.set_yticklabels(ax.get_yticklabels(), fontsize=9, fontweight="bold")

    # Clean frame
    for spine in ax.spines.values():
        spine.set_linewidth(2)
        spine.set_edgecolor("black")

    sns.despine()
    plt.tight_layout()

    outname = f"TopGenes_{comp}_{'Progression' if ATE_val>0 else 'Protective'}_SingleColor.png"
    plt.savefig(outname, dpi=600, bbox_inches="tight")
    plt.show()

    print(f"Saved: {outname}")

# ============================================================
#   PLS COMPONENT E-VALUE BARPLOT — BLUE→WHITE→RED GRADIENT
#   (Clean axis spacing + reduced font sizes)
# ============================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap
from matplotlib.ticker import MaxNLocator

# -------------------------------
# 1) LOAD + CLEAN
# -------------------------------

df = pd.read_csv("PLS_DML_Evalue_Sensitivity.csv")

# Standardize column names
df.columns = (
    df.columns.str.strip()
              .str.replace(" ", "_")
              .str.replace("-", "_")
              .str.replace("(", "")
              .str.replace(")", "")
)

ev_col = "E_value_Point"        # matches your file
df = df.sort_values(ev_col, ascending=False)

# -------------------------------
# 2) CREATE BLUE → WHITE → RED GRADIENT
# -------------------------------

df["EV_norm"] = (
    (df[ev_col] - df[ev_col].min()) /
    (df[ev_col].max() - df[ev_col].min())
)

blue_white_red = LinearSegmentedColormap.from_list(
    "blue_white_red",
    ["#08306b", "white", "#a50f15"],   # navy → white → deep red
    N=256
)

df["Color"] = df["EV_norm"].apply(lambda x: blue_white_red(x))

# -------------------------------
# 3) CREATE FIGURE + AXES
# -------------------------------

fig, ax = plt.subplots(figsize=(5, 5), dpi=600)
sns.set_theme(style="white")

# Plot bars
bars = ax.barh(
    df["Component"],
    df[ev_col],
    color=df["Color"],
    edgecolor="black",
    linewidth=1.3
)

ax.invert_yaxis()

ax.set_xlabel("E-value (Point Estimate)", fontsize=14, weight="bold")
ax.set_ylabel("")
ax.set_title("PLS Component E-values", fontsize=14, weight="bold", pad=12)

# -------------------------------
# 4) FIX AXIS CROWDING + FONT SIZES
# -------------------------------

# Reduce number of x-axis ticks
ax.xaxis.set_major_locator(MaxNLocator(5))

# Reduce tick label sizes
ax.tick_params(axis="x", labelsize=11)
ax.tick_params(axis="y", labelsize=11)

# Optional: format x-axis values to 2 decimals
ax.xaxis.set_major_formatter(lambda x, pos: f"{x:.2f}")

# -------------------------------
# 5) REMOVE GRIDLINES + STYLE SPINES
# -------------------------------

sns.despine()
for spine in ax.spines.values():
    spine.set_linewidth(1.8)
    spine.set_edgecolor("black")

# -------------------------------
# 6) ADD COLORBAR
# -------------------------------

norm = plt.Normalize(df[ev_col].min(), df[ev_col].max())
sm = plt.cm.ScalarMappable(cmap=blue_white_red, norm=norm)
sm.set_array([])

cbar = fig.colorbar(sm, ax=ax, pad=0.02)
cbar.set_label("E-value Gradient", fontsize=12, weight="bold")
cbar.ax.tick_params(labelsize=11)

# -------------------------------
# 7) SAVE FIGURE
# -------------------------------

plt.tight_layout()
plt.savefig(
    "PLS_Evalue_Barplot_BlueWhiteRed.png",
    dpi=600,
    bbox_inches="tight"
)
plt.show()

print("Saved: PLS_Evalue_Barplot_BlueWhiteRed.png")

# ============================================================
# PLS–DML ATE Boxplot — Colored by Component (Publication Style)
# ============================================================

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# -----------------------------
# Load and detect ATE column
# -----------------------------
df = pd.read_csv("DML_Bootstrap_Effects_ByComponent.csv")

possible_cols = ["ATE_boot", "ATE", "ATE_bootstrap", "ATE_mean_boot"]
ate_col = next((col for col in possible_cols if col in df.columns), None)

if ate_col is None:
    raise ValueError("No valid ATE column found in CSV.")

df = df.rename(columns={ate_col: "ATE_value"})

# -----------------------------
# Order components
# -----------------------------
df["CompNum"] = df["Component"].str.extract("(\d+)").astype(int)
df = df.sort_values("CompNum")
component_order = df["Component"].unique()

# -----------------------------
# PLS Component Colors
# -----------------------------
pls_colors = {
    "PLS_1": "#CAB2D6",
    "PLS_2": "#FB9A99",
    "PLS_3": "#33A02C",
    "PLS_4": "#6A3D9A",
    "PLS_5": "#FF7F00",
    "PLS_6": "#A6CEE3",
    "PLS_7": "#B2DF8A",
    "PLS_8": "#BEBADA",
    "PLS_9": "#1F78B4",
    "PLS_10": "#80CDC1"
}

# Map color to each row
df["Color"] = df["Component"].map(pls_colors)

# -----------------------------
# Plot
# -----------------------------
plt.figure(figsize=(5, 4), dpi=600)
sns.set_theme(style="white", context="talk")

# Boxplot for each component, colored
for comp in component_order:
    sns.boxplot(
        data=df[df["Component"] == comp],
        x="Component",
        y="ATE_value",
        color=pls_colors[comp],
        width=0.6,
        boxprops={"edgecolor": "black", "linewidth": 1.3},
        whiskerprops={"color": "black", "linewidth": 1.1},
        medianprops={"color": "black", "linewidth": 1.7},
        showcaps=True,
        fliersize=0
    )

# -----------------------------
# Format axes and annotations
# -----------------------------
plt.axhline(0, color="red", linestyle="--", linewidth=1.4)
plt.xticks(rotation=50, ha="right", fontsize=10)
plt.yticks(fontsize=12)
plt.xlabel("PLS Components", fontsize=12, weight="bold")
plt.ylabel("ATE (Bootstrap Distribution)", fontsize=12, weight="bold")

#plt.text(
    #0.98, 0.95, "n = 200 bootstraps",
    #transform=plt.gca().transAxes,
    #ha="right", fontsize=11, weight="bold"
#)

sns.despine()
plt.tight_layout()
plt.savefig("PLS_DML_ATE_Boxplot_COLORED.png", dpi=600, bbox_inches="tight")
plt.show()

print("Saved: PLS_DML_ATE_Boxplot_COLORED.png")



# ============================================================
# Gene–Species Bubble Plot for PLS_1 and PLS_9
# Top 10 species per component
# ============================================================

# -----------------------------------------
# STEP 1: Import Libraries
# -----------------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# -----------------------------------------
# STEP 2: Load Data
# -----------------------------------------
df = pd.read_csv("PLS_1_9_Gene_to_Species_Link.csv")

# -----------------------------------------
# STEP 3: Get Top Species per Component
# -----------------------------------------
def get_top_species(df, component, top_n=10):
    df_sub = df[df["Component"] == component].copy()
    top_species = (
        df_sub.groupby("Species")["Mean_TPM"]
        .sum()
        .sort_values(ascending=False)
        .head(top_n)
        .index
    )
    return df_sub[df_sub["Species"].isin(top_species)].copy()

df_pls1 = get_top_species(df, "PLS_1", top_n=10)
df_pls9 = get_top_species(df, "PLS_9", top_n=10)

# -----------------------------------------
# STEP 4: Bubble Plot Function
# -----------------------------------------
def plot_bubble_by_component(df_sub, component, filename):
    df_sub = df_sub.copy()
    df_sub["Log10_TPM"] = np.log10(df_sub["Mean_TPM"] + 1)

    plt.figure(figsize=(12, 6), dpi=300)
    bubble = sns.scatterplot(
        data=df_sub,
        x="Gene_clean",
        y="Species",
        size="Sample_Count",
        hue="Log10_TPM",
        palette="plasma",
        sizes=(40, 500),
        alpha=0.85,
        edgecolor="black",
        linewidth=0.5
    )

    #bubble.set_title(f"Gene–Species Bubble Plot: {component}", fontsize=14, weight='bold')
    bubble.set_xlabel("Gene", fontsize=12)
    bubble.set_ylabel("Species", fontsize=12)
    plt.xticks(rotation=90, ha='right', fontsize=10)
    plt.yticks(fontsize=10)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)
    plt.tight_layout()
    plt.savefig(filename, dpi=600)
    plt.close()

# -----------------------------------------
# STEP 5: Generate & Save Bubble Plots
# -----------------------------------------
plot_bubble_by_component(df_pls1, "PLS_1", "PLS_1_BubblePlot_GeneSpecies.png")
plot_bubble_by_component(df_pls9, "PLS_9", "PLS_9_BubblePlot_GeneSpecies.png")

print("Bubble plots saved as high-resolution PNG files.")



!pip install adjustText

# ============================================================
# Publication-Ready Gene Network — PLS₁ & PLS₉
# Eigenvector-centrality hubs (STRUCTURAL influence)
# Top 50 genes per component
# All nodes same size, ALL gene labels shown
# ============================================================

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from adjustText import adjust_text

# -----------------------------
# Load ranked PLS genes
# -----------------------------
RANKED_FILE = "PLS_TopGenes_Ranked_with_ComponentStats.csv"
df = pd.read_csv(RANKED_FILE)

# Keep only PLS_1 and PLS_9
df = df[df["Component"].isin(["PLS_1", "PLS_9"])].copy()

# -----------------------------
# Select TOP 50 genes per component by absolute loading
# -----------------------------
top_genes = (
    df.sort_values("Abs_Loading", ascending=False)
      .groupby("Component", group_keys=False)
      .head(50)
)

print(f"Using {top_genes.shape[0]} genes (50 per component)")

# -----------------------------
# Build rank‑proximity network
# -----------------------------
G = nx.Graph()

# Add nodes
for _, row in top_genes.iterrows():
    G.add_node(row["Gene"], component=row["Component"])

# Add edges between adjacent ranks (within each PLS)
for comp, group in top_genes.groupby("Component"):
    group = group.sort_values("Rank")
    genes = group["Gene"].tolist()
    for i in range(len(genes) - 1):
        G.add_edge(genes[i], genes[i + 1])

print(f"Network built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")

# -----------------------------
# Eigenvector centrality (ROBUST)
# -----------------------------
try:
    eig_cent = nx.eigenvector_centrality(
        G,
        max_iter=2000,
        tol=1e-06
    )
except Exception as e:
    print("⚠ Eigenvector centrality failed — using zeros:", e)
    eig_cent = {n: 0.0 for n in G.nodes()}

# Identify top 5 hubs per component
metrics = pd.DataFrame({
    "Gene": list(G.nodes()),
    "Component": [G.nodes[n]["component"] for n in G.nodes()],
    "Eigenvector": [eig_cent[n] for n in G.nodes()]
})

hub_pls1 = (
    metrics[metrics["Component"] == "PLS_1"]
    .sort_values("Eigenvector", ascending=False)
    .head(5)
)

hub_pls9 = (
    metrics[metrics["Component"] == "PLS_9"]
    .sort_values("Eigenvector", ascending=False)
    .head(5)
)

hub_genes = set(hub_pls1["Gene"]).union(set(hub_pls9["Gene"]))

print("\n Top eigenvector hubs — PLS₁:")
print(hub_pls1[["Gene", "Eigenvector"]])

print("\n Top eigenvector hubs — PLS₉:")
print(hub_pls9[["Gene", "Eigenvector"]])

# -----------------------------
# Plot styling (journal‑safe)
# -----------------------------
sns.set(style="white")
plt.rcParams.update({
    "font.family": "serif",
    "font.serif": ["DejaVu Serif"],
    "pdf.fonttype": 42,
    "ps.fonttype": 42
})

fig, ax = plt.subplots(figsize=(9, 10))

# Deterministic layout
pos = nx.spring_layout(G, k=0.35, seed=42)

# Node colors
color_map = {"PLS_1": "#E74C3C", "PLS_9": "#2ECC71"}  # red / green
node_colors = [color_map[G.nodes[n]["component"]] for n in G.nodes()]

# All nodes SAME SIZE
node_sizes = [700 for _ in G.nodes()]

# -----------------------------
# Draw edges
# -----------------------------
nx.draw_networkx_edges(
    G,
    pos,
    alpha=0.20,
    edge_color="gray",
    width=1.0,
    ax=ax
)

# -----------------------------
# Draw nodes
# -----------------------------
nx.draw_networkx_nodes(
    G,
    pos,
    node_color=node_colors,
    node_size=node_sizes,
    edgecolors="black",
    linewidths=0.7,
    alpha=0.8,
    ax=ax
)

# -----------------------------
# Highlight eigenvector hubs (gold halo)
# -----------------------------
nx.draw_networkx_nodes(
    G,
    pos,
    nodelist=list(hub_genes),
    node_color=[color_map[G.nodes[h]["component"]] for h in hub_genes],
    node_size=[1200 for _ in hub_genes],
    edgecolors="gold",
    linewidths=2.0,
    ax=ax
)

# -----------------------------
# Label ALL genes
# -----------------------------
texts = []
for g in G.nodes():
    x, y = pos[g]
    texts.append(
        plt.text(
            x, y, g,
            fontsize=7.0,
            fontweight="bold",
            ha="center",
            va="center"
        )
    )

adjust_text(
    texts,
    arrowprops=dict(arrowstyle="-", color="gray", lw=0.25),
    force_points=0.3,
    force_text=0.4,
    expand_text=(1.2, 1.3),
    expand_points=(1.2, 1.2)
)

# -----------------------------
# Final touches
# -----------------------------
plt.title(
    "PLS₁ (Red) and PLS₉ (Green) Gene Network\nTop 5 Eigenvector Hubs Highlighted",
    fontsize=13,
    fontweight="bold",
    pad=10
)

plt.axis("off")
plt.tight_layout()

plt.savefig(
    "PLS1_PLS9_EigenvectorGeneNetwork_ALLGENES.png",
    dpi=600,
    bbox_inches="tight"
)

plt.show()

print(" Saved → PLS1_PLS9_EigenvectorGeneNetwork_ALLGENES.png")

####################################################################
# ============================================================
# Save network metrics (Degree + Eigenvector centrality)
# ============================================================

# Compute degree centrality (already implicit but recomputed safely)
deg_cent = nx.degree_centrality(G)

# Build metrics table
network_metrics = pd.DataFrame({
    "Gene": list(G.nodes()),
    "Component": [G.nodes[n]["component"] for n in G.nodes()],
    "Degree_Centrality": [deg_cent[n] for n in G.nodes()],
    "Eigenvector_Centrality": [eig_cent[n] for n in G.nodes()]
})

# Save to CSV
metrics_file = "PLS1_PLS9_Network_Metrics_Top50.csv"
network_metrics.to_csv(metrics_file, index=False)

print(f"Network metrics saved → {metrics_file}")
##########################################################################
####################################################################
# ============================================================
# Save HUB gene metrics (Top 5 Eigenvector hubs per component)
# ============================================================

# Extract hub metrics only
hub_metrics = network_metrics[
    network_metrics["Gene"].isin(hub_genes)
].copy()

# Rank hubs within each component
hub_metrics["Hub_Rank_within_Component"] = (
    hub_metrics
    .groupby("Component")["Eigenvector_Centrality"]
    .rank(method="dense", ascending=False)
    .astype(int)
)

# Sort nicely
hub_metrics = hub_metrics.sort_values(
    ["Component", "Hub_Rank_within_Component"]
)

# Save hub-only table
hub_metrics_file = "PLS1_PLS9_HubGenes_Metrics_Top5.csv"
hub_metrics.to_csv(hub_metrics_file, index=False)

print(f"Hub gene metrics saved → {hub_metrics_file}")





# ============================================================
# PLS₁ + PLS₉ Gene-Based LOCO Classification (FINAL – OPTIMIZED)
# Models: LR, DT, RF, GB, SVM
# Evaluation: Nested CV + Holdout + LOCO
# Selection: Accuracy + Balanced Accuracy + Macro-MCC
# ============================================================

# -----------------------------
# IMPORTS
# -----------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, f1_score,
    matthews_corrcoef, roc_curve, auc,
    classification_report, confusion_matrix
)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier

# -----------------------------
# LOAD DATA
# -----------------------------
X_full = pd.read_csv("ARG_VF_TPM_clr_batch_corrected_v2.csv", index_col=0)
meta = pd.read_csv("ARG_VF_metadata_final_matched.csv").set_index("Sample_ID")

X_full = X_full.loc[meta.index]
y_raw = meta["Group"]
cohort = meta["Project"]

# -----------------------------
# ENCODE LABELS
# -----------------------------
le = LabelEncoder()
y = le.fit_transform(y_raw)

print("\nClass encoding:")
for i, cls in enumerate(le.classes_):
    print(f"{cls} → {i}")

# -----------------------------
# FILTER TO PLS₁ + PLS₉ GENES
# -----------------------------
pls = pd.read_csv("PLS_TopGenes_Ranked_with_ComponentStats.csv")
pls_genes = pls[pls["Component"].isin(["PLS_1", "PLS_9"])]["Gene"].unique()
X = X_full.loc[:, X_full.columns.intersection(pls_genes)]

print(f"\n Using {X.shape[1]} PLS genes")

# -----------------------------
# MACRO-MCC
# -----------------------------
def macro_mcc(y_true, y_pred):
    return np.mean([
        matthews_corrcoef((y_true == c).astype(int),
                          (y_pred == c).astype(int))
        for c in np.unique(y_true)
    ])

# -----------------------------
# COARSE GRIDS (NESTED CV ONLY)
# -----------------------------
coarse_models = {

    "LR": (
        LogisticRegression(max_iter=2000, class_weight="balanced"),
        {"clf__C": [0.01, 1, 100]}
    ),

    "DT": (
        DecisionTreeClassifier(random_state=42, class_weight="balanced"),
        {"clf__max_depth": [5, 15, None],
         "clf__min_samples_leaf": [1, 5]}
    ),

    "RF": (
        RandomForestClassifier(random_state=42, n_jobs=-1, class_weight="balanced"),
        {"clf__n_estimators": [200, 500],
         "clf__max_depth": [None, 20],
         "clf__max_features": ["sqrt", 0.3]}
    ),

    "GB": (
        GradientBoostingClassifier(random_state=42),
        {"clf__n_estimators": [100, 300],
         "clf__learning_rate": [0.05, 0.1],
         "clf__max_depth": [2, 3]}
    ),

    "SVM": (
        SVC(kernel="rbf", probability=True, class_weight="balanced"),
        {"clf__C": [0.1, 1, 10],
         "clf__gamma": [1e-3, 1e-2]}
    )
}

# -----------------------------
# FULL GRIDS (FINAL TUNING ONLY)
# -----------------------------
full_grids = {

    "LR": {"clf__C": np.logspace(-3, 2, 6)},

    "DT": {
        "clf__max_depth": [3, 5, 10, 20, None],
        "clf__min_samples_split": [2, 5, 10],
        "clf__min_samples_leaf": [1, 2, 5]
    },

    "RF": {
        "clf__n_estimators": [200, 300, 500, 800],
        "clf__max_depth": [None, 10, 30],
        "clf__min_samples_split": [2, 5, 10],
        "clf__min_samples_leaf": [1, 2, 5],
        "clf__max_features": ["sqrt", "log2", 0.3]
    },

    "GB": {
        "clf__n_estimators": [100, 300, 600],
        "clf__learning_rate": [0.01, 0.05, 0.1],
        "clf__max_depth": [2, 3, 5],
        "clf__subsample": [0.7, 0.9, 1.0]
    },

    "SVM": {
        "clf__C": np.logspace(-2, 2, 5),
        "clf__gamma": np.logspace(-4, -1, 4)
    }
}

# -----------------------------
# HOLDOUT SPLIT
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# -----------------------------
# NESTED CV (MODEL FAMILY SELECTION)
# -----------------------------
outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
results = []

for name, (clf, grid) in coarse_models.items():
    pipe = Pipeline([("scaler", StandardScaler()), ("clf", clf)])
    gs = GridSearchCV(pipe, grid, cv=3, scoring="f1_macro", n_jobs=-1)

    accs, baccs, f1s, mccs, mmccs = [], [], [], [], []

    for tr, va in outer_cv.split(X_train, y_train):
        gs.fit(X_train.iloc[tr], y_train[tr])
        pred = gs.best_estimator_.predict(X_train.iloc[va])

        accs.append(accuracy_score(y_train[va], pred))
        baccs.append(balanced_accuracy_score(y_train[va], pred))
        f1s.append(f1_score(y_train[va], pred, average="macro"))
        mccs.append(matthews_corrcoef(y_train[va], pred))
        mmccs.append(macro_mcc(y_train[va], pred))

    selection_score = (
        np.mean(accs)
        + np.mean(baccs)
        + np.mean(mmccs)
    ) / 3.0

    results.append({
        "Model": name,
        "Accuracy": np.mean(accs),
        "Balanced_Accuracy": np.mean(baccs),
        "Macro_F1": np.mean(f1s),
        "MCC": np.mean(mccs),
        "Macro_MCC": np.mean(mmccs),
        "Selection_Score": selection_score
    })

bench_df = pd.DataFrame(results)
bench_df.to_csv("Model_Benchmark_NestedCV.csv", index=False)

print("\n Nested CV benchmark:")
print(bench_df)

best_model_name = (
    bench_df.sort_values("Selection_Score", ascending=False)
    .iloc[0]["Model"]
)

print(
    f"\n Best model family "
    f"(Accuracy + Balanced Accuracy + Macro-MCC): {best_model_name}"
)

# -----------------------------
# FINAL TUNING (FULL GRID)
# -----------------------------
final_pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", coarse_models[best_model_name][0])
])

final_gs = GridSearchCV(
    final_pipe,
    full_grids[best_model_name],
    cv=5,
    scoring="f1_macro",
    n_jobs=-1
)

final_gs.fit(X_train, y_train)
best_estimator = final_gs.best_estimator_

print("\n Best hyperparameters:")
print(final_gs.best_params_)

pd.DataFrame([final_gs.best_params_]).to_csv(
    "Best_Model_Hyperparameters.csv", index=False
)

# -----------------------------
# HOLDOUT EVALUATION
# -----------------------------
y_test_pred = best_estimator.predict(X_test)

pd.DataFrame([{
    "Accuracy": accuracy_score(y_test, y_test_pred),
    "Balanced_Accuracy": balanced_accuracy_score(y_test, y_test_pred),
    "Macro_F1": f1_score(y_test, y_test_pred, average="macro"),
    "MCC": matthews_corrcoef(y_test, y_test_pred),
    "Macro_MCC": macro_mcc(y_test, y_test_pred)
}]).to_csv("Holdout_Performance.csv", index=False)

# -----------------------------
# PER-CLASS METRICS
# -----------------------------
pd.DataFrame(
    classification_report(
        y_test, y_test_pred,
        target_names=le.classes_,
        output_dict=True
    )
).transpose().loc[le.classes_].to_csv("Holdout_PerClass_Metrics.csv")

# -----------------------------
# CONFUSION MATRIX
# -----------------------------
cm = confusion_matrix(y_test, y_test_pred)
plt.figure()
plt.imshow(cm)
plt.colorbar()
plt.xticks(range(len(le.classes_)), le.classes_, rotation=45)
plt.yticks(range(len(le.classes_)), le.classes_)
plt.title("Confusion Matrix - Holdout")
plt.tight_layout()
plt.savefig("Confusion_Matrix_Holdout.png", dpi=300)
plt.show()

# -----------------------------
# ROC CURVES
# -----------------------------
y_test_bin = label_binarize(y_test, classes=np.unique(y))
y_score = OneVsRestClassifier(best_estimator).fit(
    X_train, y_train
).predict_proba(X_test)

plt.figure()
for i, cls in enumerate(le.classes_):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    plt.plot(fpr, tpr, label=f"{cls} (AUC={auc(fpr, tpr):.2f})")

plt.plot([0, 1], [0, 1], "k--")
plt.legend()
plt.title("ROC Curve - Holdout")
plt.tight_layout()
plt.savefig("ROC_Curve_Holdout.png", dpi=300)
plt.show()

# -----------------------------
# LOCO (FIXED HYPERPARAMETERS)
# -----------------------------
loco_results = []

for c in cohort.unique():
    mask = cohort == c
    if len(np.unique(y[mask])) < 2:
        continue

    best_estimator.fit(X[~mask], y[~mask])
    pred = best_estimator.predict(X[mask])

    loco_results.append({
        "Left_Out_Project": c,
        "Accuracy": accuracy_score(y[mask], pred),
        "Balanced_Accuracy": balanced_accuracy_score(y[mask], pred),
        "Macro_F1": f1_score(y[mask], pred, average="macro"),
        "MCC": matthews_corrcoef(y[mask], pred),
        "Macro_MCC": macro_mcc(y[mask], pred),
        "N_test": mask.sum()
    })

pd.DataFrame(loco_results).to_csv("LOCO_Performance.csv", index=False)

print("\n PIPELINE COMPLETE — FINAL, CORRECT, AND REVIEWER-READY")



# ============================================================
# PLS₁ vs PLS₉ vs PLS₁+PLS₉
# LOCO Evaluation + Feature Stability Analysis
# Model: Gradient Boosting (best tuned from nested CV)
# ============================================================

import pandas as pd
import numpy as np
from collections import defaultdict

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    balanced_accuracy_score,
    f1_score,
    matthews_corrcoef
)

# ------------------------------------------------------------
# LOAD DATA
# ------------------------------------------------------------
X_full = pd.read_csv(
    "ARG_VF_TPM_clr_batch_corrected_v2.csv",
    index_col=0
)

meta = pd.read_csv("ARG_VF_metadata_final_matched.csv")
meta = meta.set_index("Sample_ID")

# Align samples
X_full = X_full.loc[meta.index]

# Labels and LOCO variable
y_raw = meta["Group"]
cohort = meta["Project"]

# Encode labels
le = LabelEncoder()
y = le.fit_transform(y_raw)

print("Class encoding:")
for i, cls in enumerate(le.classes_):
    print(f"{cls} → {i}")

# ------------------------------------------------------------
# LOAD PLS GENE SETS
# ------------------------------------------------------------
pls = pd.read_csv("PLS_TopGenes_Ranked_with_ComponentStats.csv")

genes_pls1 = pls[pls["Component"] == "PLS_1"]["Gene"].unique()
genes_pls9 = pls[pls["Component"] == "PLS_9"]["Gene"].unique()
genes_pls1_9 = np.unique(np.concatenate([genes_pls1, genes_pls9]))

feature_sets = {
    "PLS1_only": genes_pls1,
    "PLS9_only": genes_pls9,
    "PLS1_PLS9": genes_pls1_9
}

# ------------------------------------------------------------
# MACRO-MCC FUNCTION
# ------------------------------------------------------------
def macro_mcc(y_true, y_pred):
    labels = np.unique(y_true)
    return np.mean([
        matthews_corrcoef(
            (y_true == lab).astype(int),
            (y_pred == lab).astype(int)
        )
        for lab in labels
    ])

# ------------------------------------------------------------
# FIXED, BEST-TUNED GRADIENT BOOSTING MODEL
# (from your nested CV results)
# ------------------------------------------------------------
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=1.0,
    random_state=42
)

# ------------------------------------------------------------
# RUN LOCO + FEATURE STABILITY
# ------------------------------------------------------------
TOP_K = 20
results = []
stability = defaultdict(lambda: defaultdict(int))

for fs_name, genes in feature_sets.items():
    print(f"\n Running LOCO for {fs_name} ({len(genes)} genes)")

    X = X_full.loc[:, X_full.columns.intersection(genes)]

    for c in cohort.unique():
        test_mask = cohort == c

        # Skip cohorts with single class
        if len(np.unique(y[test_mask])) < 2:
            print(f"⚠ Skipping {c} (single class)")
            continue

        X_tr = X[~test_mask]
        y_tr = y[~test_mask]
        X_te = X[test_mask]
        y_te = y[test_mask]

        pipe = Pipeline([
            ("scaler", StandardScaler()),
            ("gb", gb)
        ])

        pipe.fit(X_tr, y_tr)
        y_pred = pipe.predict(X_te)

        # ------------------------
        # Metrics
        # ------------------------
        results.append({
            "Feature_Set": fs_name,
            "Left_Out_Project": c,
            "Balanced_Accuracy": balanced_accuracy_score(y_te, y_pred),
            "Macro_F1": f1_score(y_te, y_pred, average="macro"),
            "MCC": matthews_corrcoef(y_te, y_pred),
            "Macro_MCC": macro_mcc(y_te, y_pred),
            "N_test": len(y_te)
        })

        # ------------------------
        # Feature stability
        # ------------------------
        importances = pipe.named_steps["gb"].feature_importances_
        feature_names = X.columns

        top_features = (
            pd.Series(importances, index=feature_names)
            .sort_values(ascending=False)
            .head(TOP_K)
            .index
        )

        for g in top_features:
            stability[fs_name][g] += 1

# ------------------------------------------------------------
# SAVE LOCO RESULTS
# ------------------------------------------------------------
results_df = pd.DataFrame(results)
results_df.to_csv("PLS_Component_LOCO_Comparison_GB.csv", index=False)

print("\n Saved: PLS_Component_LOCO_Comparison_GB.csv")

# ------------------------------------------------------------
# SAVE FEATURE STABILITY TABLES
# ------------------------------------------------------------
for fs_name in stability:
    stab_df = (
        pd.DataFrame.from_dict(
            stability[fs_name],
            orient="index",
            columns=["TopK_Count"]
        )
        .sort_values("TopK_Count", ascending=False)
    )

    out_file = f"{fs_name}_Feature_Stability_GB.csv"
    stab_df.to_csv(out_file)

    print(f" Saved: {out_file}")

print("\n ANALYSIS COMPLETE (GB-based PLS comparison)")





# ============================================================
# Feature Stability Plots (Styled)
# PLS₁ = Red | PLS₉ = Green | PLS₁+PLS₉ = Violet
# Thick bar edges + thick plot borders
# ============================================================

import pandas as pd
import matplotlib.pyplot as plt

TOP_N = 20  # number of genes to display
EDGE_WIDTH = 2
SPINE_WIDTH = 2

# ------------------------------------------------------------
# Helper function to style axes
# ------------------------------------------------------------
def style_axes(ax):
    for spine in ax.spines.values():
        spine.set_linewidth(SPINE_WIDTH)
    ax.tick_params(width=SPINE_WIDTH)

# ------------------------------------------------------------
# Plot function
# ------------------------------------------------------------
def plot_feature_stability(csv_file, title, color, output_file, top_n=15):
    df = pd.read_csv(csv_file, index_col=0)
    df = df.sort_values("TopK_Count", ascending=False).head(top_n)

    fig, ax = plt.subplots(figsize=(5, 5))

    ax.barh(
        df.index[::-1],
        df["TopK_Count"][::-1],
        color=color,
        edgecolor="black",
        linewidth=EDGE_WIDTH
    )

    ax.set_xlabel("Number of LOCO folds in Top-K", fontsize=11)
    ax.set_title(title, fontsize=12)

    style_axes(ax)
    plt.tight_layout()
    plt.savefig(output_file, dpi=600)
    plt.show()


# ------------------------------------------------------------
# PLS₁ (RED)
# ------------------------------------------------------------
plot_feature_stability(
    csv_file="PLS1_only_Feature_Stability_GB.csv",
    title="",
    color="red",
    output_file="PLS1_Feature_Stability_Styled.png",
    top_n=TOP_N
)

# ------------------------------------------------------------
# PLS₉ (GREEN)
# ------------------------------------------------------------
plot_feature_stability(
    csv_file="PLS9_only_Feature_Stability_GB.csv",
    title="",
    color="green",
    output_file="PLS9_Feature_Stability_Styled.png",
    top_n=TOP_N
)

# ------------------------------------------------------------
# PLS₁ + PLS₉ (VIOLET)
# ------------------------------------------------------------
plot_feature_stability(
    csv_file="PLS1_PLS9_Feature_Stability_GB.csv",
    title="",
    color="purple",
    output_file="PLS1_PLS9_Feature_Stability_Styled.png",
    top_n=TOP_N
)

print("\n Styled feature stability plots generated successfully")





# ============================================================
#  CFSI (PLS₁ − PLS₉) + Sample-Level & Gene-Level Power Analysis
# Association-based, reviewer-safe, publication-ready
# ============================================================

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import zscore
from statsmodels.stats.power import FTestAnovaPower
import re

# ============================================================
# Helper: canonical gene name (ROBUST)
# ============================================================
def canonical_gene_name(s):
    s = s.lower()
    s = re.sub(r"^(arg_|vf_)", "", s)
    s = s.replace("__", "_")
    s = s.replace("-", "_")
    s = s.replace("(", "")
    s = s.replace(")", "")
    s = re.sub(r"_+", "_", s)
    return s.strip("_")

GROUPS = ["Healthy", "Adenoma", "Cancer"]
alpha = 0.05
target_power = 0.80
analysis = FTestAnovaPower()

# ============================================================
# Helper: Cohen's f from group arrays
# returns (Cohen's f, eta squared)
# ============================================================
def cohen_f_from_groups(groups):
    means = [np.mean(g) for g in groups]
    gm = np.mean(np.concatenate(groups))
    ss_between = sum(len(g) * (m - gm)**2 for g, m in zip(groups, means))
    ss_within  = sum(((g - np.mean(g))**2).sum() for g in groups)
    eta_sq = ss_between / (ss_between + ss_within)
    f = np.sqrt(eta_sq / (1 - eta_sq))
    return f, eta_sq

# ============================================================
# PART A — SAMPLE-LEVEL CFSI
# ============================================================

pls_scores = pd.read_csv("Stable_PLS_Scores.csv")
meta = pd.read_csv("ARG_VF_metadata_final_matched.csv")

df = pd.merge(
    pls_scores[["Sample_ID", "PLS_1", "PLS_9"]],
    meta[["Sample_ID", "Group"]],
    on="Sample_ID",
    how="inner"
)

df = df[df["Group"].isin(GROUPS)].copy()
df["Group"] = pd.Categorical(df["Group"], categories=GROUPS, ordered=True)

print(f"✔ CFSI samples: {df.shape[0]}")
print(df["Group"].value_counts())

# -----------------------------
# Compute CFSI
# -----------------------------
df["PLS1_z"] = zscore(df["PLS_1"])
df["PLS9_z"] = zscore(df["PLS_9"])
df["CFSI"]   = df["PLS1_z"] - df["PLS9_z"]

df.to_csv("CFSI_BySample.csv", index=False)
print("✔ Saved → CFSI_BySample.csv")

# ============================================================
# PART B — CFSI VISUALIZATION (PUBLICATION)
# ============================================================

sns.set(style="white", context="talk")

plt.figure(figsize=(5, 5), dpi=600)
ax = sns.boxplot(
    data=df,
    x="Group",
    y="CFSI",
    order=GROUPS,
    palette=["#4C72B0", "#DD8452", "#55A868"],
    showfliers=False
)

sns.stripplot(
    data=df,
    x="Group",
    y="CFSI",
    order=GROUPS,
    color="black",
    size=3,
    alpha=0.5
)

plt.axhline(0, color="red", linestyle="--", linewidth=1.2)
plt.xlabel("")
plt.ylabel("CFSI (z(PLS₁) − z(PLS₉))", weight="bold", fontsize = 12)
#plt.title("Composite Functional Shift Index Across CRC Groups", weight="bold")

for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor("black")

sns.despine()
plt.tight_layout()
plt.savefig("CFSI_ByGroup_Boxplot.png", dpi=600, bbox_inches="tight")
plt.show()

# ============================================================
# PART C — SAMPLE-LEVEL POWER ANALYSIS (CFSI)
# ============================================================

healthy = df.loc[df["Group"] == "Healthy", "CFSI"].values
adenoma = df.loc[df["Group"] == "Adenoma", "CFSI"].values
cancer  = df.loc[df["Group"] == "Cancer",  "CFSI"].values

f_cfsi, eta_cfsi = cohen_f_from_groups([healthy, adenoma, cancer])

n_req = analysis.solve_power(
    effect_size=f_cfsi,
    k_groups=3,
    alpha=alpha,
    power=target_power
)

n_actual = df.shape[0]
power_achieved = analysis.power(
    effect_size=f_cfsi,
    nobs=n_actual,
    alpha=alpha,
    k_groups=3
)

summary_cfsi = pd.DataFrame({
    "Index": ["CFSI (PLS₁ − PLS₉)"],
    "Cohen_f": [f_cfsi],
    "Eta_Squared": [eta_cfsi],
    "Required_Total_N_80pwr": [np.ceil(n_req)],
    "Required_PerGroup_N_80pwr": [np.ceil(n_req / 3)],
    "Achieved_Power_CurrentN": [power_achieved],
    "Actual_Total_N": [n_actual]
})

summary_cfsi.to_csv("CFSI_Power_Summary.csv", index=False)
print(" Saved → CFSI_Power_Summary.csv")

# -----------------------------
# Power curve (publication)
# -----------------------------
samples = np.arange(30, 400, 5)
power_curve = analysis.power(
    effect_size=f_cfsi,
    nobs=samples,
    alpha=alpha,
    k_groups=3
)

plt.figure(figsize=(5,5), dpi=600)
plt.plot(samples, power_curve, lw=2.8, color="#1f77b4",
         label=f"CFSI (f = {f_cfsi:.2f})")
plt.axhline(0.8, color="red", ls="--", lw=1.5, label="80% power")

plt.xlabel("Total Sample Size", weight="bold", fontsize = 12)
plt.ylabel("Statistical Power", weight="bold", fontsize = 12)
plt.legend(frameon=True)

ax = plt.gca()
ax.grid(False)
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor("black")

plt.tight_layout()
plt.savefig("CFSI_PowerCurve.png", dpi=600, bbox_inches="tight")
plt.show()

# ============================================================
# PART D — GENE-LEVEL POWER ANALYSIS (PLS₁ & PLS₉)
# NOTE: Required sample size reflects detectability,
#       NOT biological importance
# ============================================================

expr = pd.read_csv("ARG_VF_TPM_clr_batch_corrected_v2.csv", index_col=0)
expr.columns = [canonical_gene_name(c) for c in expr.columns]

meta2 = meta.set_index("Sample_ID").loc[expr.index]
meta2 = meta2[meta2["Group"].isin(GROUPS)].copy()
expr = expr.loc[meta2.index]

pls = pd.read_csv("PLS_TopGenes_Ranked_with_ComponentStats.csv")
pls = pls[pls["Component"].isin(["PLS_1", "PLS_9"])].copy()
pls["Gene_canon"] = pls["Gene"].apply(canonical_gene_name)

genes = sorted(set(pls["Gene_canon"]).intersection(expr.columns))
print(f" Using {len(genes)} PLS₁/PLS₉ genes")

records = []

for gene in genes:
    y = expr[gene]
    g = meta2["Group"]

    arrays = [
        y[g=="Healthy"].values,
        y[g=="Adenoma"].values,
        y[g=="Cancer"].values
    ]

    if any(len(a) < 2 for a in arrays):
        continue

    f, eta = cohen_f_from_groups(arrays)

    if not np.isfinite(f) or f <= 0:
        continue

    try:
        n_req = analysis.solve_power(
            effect_size=f,
            k_groups=3,
            alpha=alpha,
            power=target_power
        )
    except Exception:
        n_req = np.nan

    records.append({
        "Gene": gene,
        "Component": pls.loc[pls["Gene_canon"]==gene, "Component"].iloc[0],
        "Cohen_f": f,
        "Required_PerGroup_N_80pwr": np.ceil(n_req/3) if np.isfinite(n_req) else np.nan
    })

df_gene_power = pd.DataFrame(records).sort_values(
    ["Component","Required_PerGroup_N_80pwr"]
)

df_gene_power.to_csv("PLS1_PLS9_GeneLevel_Power_ANOVA.csv", index=False)
print(" Saved → PLS1_PLS9_GeneLevel_Power_ANOVA.csv")

# -----------------------------
# Gene-level power plot
# -----------------------------
plt.figure(figsize=(5,5))
ax = sns.boxplot(
    data=df_gene_power,
    x="Component",
    y="Required_PerGroup_N_80pwr",
    palette=["#E74C3C", "#2ECC71"],
    showfliers=False
)

sns.stripplot(
    data=df_gene_power,
    x="Component",
    y="Required_PerGroup_N_80pwr",
    color="black",
    size=3,
    alpha=0.5
)

#  LOG SCALE
ax.set_yscale("log")

plt.ylabel("Required Sample Size/Group (80% power)", weight="bold", fontsize = 12)

# Styling (consistent with your other figures)
for spine in ax.spines.values():
    spine.set_linewidth(2)
    spine.set_edgecolor("black")

sns.despine()
plt.tight_layout()
plt.savefig(
    "PLS1_PLS9_GeneLevel_Power_Distribution_log.png",
    dpi=600,
    bbox_inches="tight"
)
plt.show()



# ============================================================
# OPTION A (CORRECTED): CFSI vs LOCO Performance
# Cohort / Project-level correlation
# ============================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr

# -----------------------------
# LOAD DATA
# -----------------------------
cfsi = pd.read_csv("CFSI_BySample.csv")
meta = pd.read_csv("ARG_VF_metadata_final_matched.csv")
loco = pd.read_csv("PLS_Component_LOCO_Comparison_GB.csv")

# -----------------------------
# ATTACH PROJECT TO CFSI
# -----------------------------
cfsi = cfsi.merge(
    meta[["Sample_ID", "Project"]],
    on="Sample_ID",
    how="left"
)

assert cfsi["Project"].isna().sum() == 0, " Missing Project labels!"

# -----------------------------
# COMPUTE PROJECT-LEVEL CFSI
# -----------------------------
cfsi_proj = (
    cfsi
    .groupby("Project")
    .agg(
        Mean_CFSI=("CFSI", "mean"),
        Mean_Abs_CFSI=("CFSI", lambda x: np.mean(np.abs(x))),
        N_samples=("CFSI", "size")
    )
    .reset_index()
)

print("\n CFSI summary per project:")
print(cfsi_proj)

# -----------------------------
# AGGREGATE LOCO PERFORMANCE
# -----------------------------
loco_summary = (
    loco
    .groupby("Left_Out_Project")
    .agg(
        Mean_BalAcc=("Balanced_Accuracy", "mean"),
        Mean_Macro_MCC=("Macro_MCC", "mean"),
        N_test=("N_test", "mean")
    )
    .reset_index()
)

print("\nLOCO summary per project:")
print(loco_summary)

# -----------------------------
# MERGE (THIS WILL NOW WORK)
# -----------------------------
df = loco_summary.merge(
    cfsi_proj,
    left_on="Left_Out_Project",
    right_on="Project",
    how="inner"
)

print("\nMerged cohort-level data:")
print(df)

assert df.shape[0] > 2, " Too few cohorts for correlation!"

# -----------------------------
# CORRELATION ANALYSIS
# -----------------------------
rho_ba, p_ba = spearmanr(df["Mean_Abs_CFSI"], df["Mean_BalAcc"])
rho_mcc, p_mcc = spearmanr(df["Mean_Abs_CFSI"], df["Mean_Macro_MCC"])

print("\nSpearman correlations:")
print(f"Balanced Accuracy vs |CFSI|: rho = {rho_ba:.2f}, p = {p_ba:.3g}")
print(f"Macro-MCC vs |CFSI|:        rho = {rho_mcc:.2f}, p = {p_mcc:.3g}")

# -----------------------------
# VISUALIZATION
# -----------------------------
sns.set(style="white", context="talk")

plt.figure(figsize=(5,5), dpi=600)
ax = sns.regplot(
    data=df,
    x="Mean_Abs_CFSI",
    y="Mean_BalAcc",
    scatter_kws={"s": 70, "color": "black"},
    line_kws={"lw": 2}
)

plt.xlabel("Mean |CFSI| per Cohort", weight="bold", fontsize=11)
plt.ylabel("LOCO Balanced Accuracy", weight="bold", fontsize=11)
plt.title(
    f"|CFSI| vs LOCO Performance\nSpearman ρ = {rho_ba:.2f}, p = {p_ba:.3g}",
    fontsize=11,
    weight="bold"
)

for spine in ax.spines.values():
    spine.set_linewidth(2)

plt.tight_layout()
plt.savefig("CFSI_vs_LOCO_BalAcc.png", dpi=600, bbox_inches="tight")
plt.show()



# ============================================================
# OPTION B (CORRECTED): Core Stable & Powered Gene Set
# ============================================================

import pandas as pd
import re

# -----------------------------
# PARAMETERS (explicit, defensible)
# -----------------------------
MIN_TOPK = 6
MAX_N_PER_GROUP = 200

# -----------------------------
# Helper: canonical gene name
# -----------------------------
def canonical_gene_name(s):
    s = s.lower()
    s = re.sub(r"^(arg_|vf_)", "", s)
    s = s.replace("__", "_")
    s = s.replace("-", "_")
    s = s.replace("(", "")
    s = s.replace(")", "")
    s = re.sub(r"_+", "_", s)
    return s.strip("_")

# -----------------------------
# LOAD DATA
# -----------------------------
stab_pls1 = pd.read_csv("PLS1_only_Feature_Stability_GB.csv", index_col=0)
stab_pls9 = pd.read_csv("PLS9_only_Feature_Stability_GB.csv", index_col=0)
gene_power = pd.read_csv("PLS1_PLS9_GeneLevel_Power_ANOVA.csv")

# -----------------------------
# PREPARE STABILITY TABLE
# -----------------------------
stab_pls1 = stab_pls1.reset_index().rename(columns={"index": "Gene_raw"})
stab_pls9 = stab_pls9.reset_index().rename(columns={"index": "Gene_raw"})

stab_pls1["Component"] = "PLS_1"
stab_pls9["Component"] = "PLS_9"

stab = pd.concat([stab_pls1, stab_pls9], ignore_index=True)

# Canonicalize gene names for merging
stab["Gene_canon"] = stab["Gene_raw"].apply(canonical_gene_name)

# -----------------------------
# PREPARE POWER TABLE
# -----------------------------
# Ensure consistent naming
gene_power = gene_power.rename(columns={"Gene": "Gene_canon"})

# -----------------------------
# MERGE (THIS WILL NOW WORK)
# -----------------------------
df = stab.merge(
    gene_power,
    on=["Gene_canon", "Component"],
    how="inner"
)

print(f"\n Matched genes after merge: {df.shape[0]}")

# -----------------------------
# FILTER CORE GENES
# -----------------------------
core_genes = df[
    (df["TopK_Count"] >= MIN_TOPK) &
    (df["Required_PerGroup_N_80pwr"] <= MAX_N_PER_GROUP)
].copy()

core_genes = core_genes.sort_values(
    ["Component", "TopK_Count", "Required_PerGroup_N_80pwr"],
    ascending=[True, False, True]
)

# -----------------------------
# CLEAN OUTPUT COLUMNS
# -----------------------------
core_genes = core_genes[
    [
        "Gene_raw",
        "Gene_canon",
        "Component",
        "TopK_Count",
        "Cohen_f",
        "Required_PerGroup_N_80pwr"
    ]
].rename(columns={"Gene_raw": "Gene"})

# -----------------------------
# SAVE RESULTS
# -----------------------------
core_genes.to_csv("Core_PLS_Genes_Stable_Powered.csv", index=False)

print("\n Core gene set saved → Core_PLS_Genes_Stable_Powered.csv")
print("\n Summary by component:")
print(core_genes.groupby("Component").size())
