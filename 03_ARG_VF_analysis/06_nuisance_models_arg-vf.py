# -*- coding: utf-8 -*-
"""ML_ARG-VF_genes_Benchmarking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13wu3xDOyEIPqU7YMmiFrETMS-0cuc0fu
"""

# ============================================================
# MACHINE LEARNING BENCHMARKING ‚Äî GENE-LEVEL (ARG + VF)
# 10-FOLD CV ONLY ‚Äî CLEAN & PUBLICATION-READY
# ============================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, f1_score,
    cohen_kappa_score, matthews_corrcoef
)
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import make_scorer

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

import warnings
warnings.filterwarnings("ignore")

print("\n==============================")
print("  ML Benchmarking on GENES")
print("==============================\n")

# ============================================================
# STEP 1 ‚Äî Load TPM + Metadata
# ============================================================

tpm = pd.read_csv("ARG_VF_TPM_clr_batch_corrected_v2.csv", index_col=0)
meta = pd.read_csv("ARG_VF_metadata_final_matched.csv")

# Clean sample IDs
tpm.index = tpm.index.astype(str).str.strip()
meta["Sample_ID"] = meta["Sample_ID"].astype(str).str.strip()

# Detect orientation
if any(meta["Sample_ID"].isin(tpm.index)):
    print("‚úî Correct orientation: rows = samples, columns = genes")
else:
    print("‚úî TPM appears transposed ‚Üí Fixing")
    tpm = tpm.T

# Align samples
common = tpm.index.intersection(meta["Sample_ID"])
if len(common) == 0:
    raise ValueError("‚ùå No overlapping Sample_ID between TPM and metadata.")

tpm = tpm.loc[common]
meta = meta.set_index("Sample_ID").loc[common]

X = tpm.values
y = meta["Group"].values

print("‚úî Final dataset shape:", X.shape)
print("‚úî Classes:", np.unique(y))


# ============================================================
# STEP 2 ‚Äî Macro-MCC function
# ============================================================

def macro_mcc(y_true, y_pred):
    lb = LabelBinarizer()
    lb.fit(y_true)
    Yt = lb.transform(y_true)
    Yp = lb.transform(y_pred)

    scores = []
    for i in range(Yt.shape[1]):
        scores.append(matthews_corrcoef(Yt[:, i], Yp[:, i]))
    return np.mean(scores)


# ============================================================
# STEP 3 ‚Äî Define models & scoring metrics
# ============================================================

scorers = {
    "accuracy": make_scorer(accuracy_score),
    "balanced_accuracy": make_scorer(balanced_accuracy_score),
    "f1_macro": make_scorer(f1_score, average="macro"),
    "kappa": make_scorer(cohen_kappa_score),
    "mcc": make_scorer(matthews_corrcoef),
    "macro_mcc": make_scorer(macro_mcc),
}

models = {
    "DecisionTree": DecisionTreeClassifier(class_weight="balanced", random_state=42),
    "RandomForest": RandomForestClassifier(class_weight="balanced", n_jobs=-1, random_state=42),
    "LogisticRegression": LogisticRegression(max_iter=5000, class_weight="balanced", random_state=42),
    "SVM": SVC(kernel="rbf", probability=True, class_weight="balanced", random_state=42),
    "GradientBoosting": GradientBoostingClassifier(random_state=42)
}


# ============================================================
# STEP 4 ‚Äî 10-Fold Cross-Validation Benchmarking
# ============================================================

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
records = []

print("\n===== Running 10-Fold CV =====")

for name, model in models.items():
    print(f" ‚Üí {name}")
    cv_out = cross_validate(model, X, y, cv=cv, scoring=scorers, n_jobs=-1)

    for metric in scorers.keys():
        records.append({
            "Model": name,
            "Metric": metric,
            "Mean": np.mean(cv_out[f"test_{metric}"]),
            "SD": np.std(cv_out[f"test_{metric}"])
        })

df_results = pd.DataFrame(records)
df_results.to_csv("ML_Genes_10Fold_Benchmark.csv", index=False)

print("\n===== CV SUMMARY =====")
print(df_results.pivot(index="Model", columns="Metric", values="Mean"))

print("\n‚úî Saved: ML_Genes_10Fold_Benchmark.csv")
print("üéâ Gene-level ML benchmarking completed.\n")

# ============================================================
# STEP 9 ‚Äî SAVE STATISTICS TABLE (Mean ¬± SD) ‚Äî PUBLICATION FORMAT
# ============================================================

print("\n===== SAVING BENCHMARK STATISTICS TABLE =====")

table_mean = df_results.pivot(index="Model", columns="Metric", values="Mean")
table_sd   = df_results.pivot(index="Model", columns="Metric", values="SD")

stats_table = table_mean.round(3).astype(str) + " ¬± " + table_sd.round(3).astype(str)
stats_table.to_csv("ML_Genes_Benchmark_Statistics.csv")

print("‚úî Saved: ML_Genes_Benchmark_Statistics.csv")

# ============================================================
# STEP 10 ‚Äî 3√ó3 NATURE-STYLE BENCHMARK FIGURE
# ============================================================

print("\n===== GENERATING 3√ó3 BENCHMARK FIGURE =====")

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

df_plot = df_results.copy()
df_plot["Metric"] = df_plot["Metric"].str.replace("_", " ").str.title()
df_plot["Metric"] = df_plot["Metric"].replace({
    "Mcc": "MCC",
    "Macro Mcc": "Macro-MCC"
})

metric_order = ["Accuracy", "Balanced Accuracy", "F1 Macro",
                "Kappa", "MCC", "Macro-MCC"]

model_order  = ["DecisionTree", "RandomForest",
                "LogisticRegression", "SVM", "GradientBoosting"]

fig, axes = plt.subplots(3, 3, figsize=(16, 14), dpi=350)
axes = axes.flatten()

sns.set_theme(style="white", context="talk")

for i, metric in enumerate(metric_order):
    ax = axes[i]
    df_sub = df_plot[df_plot["Metric"] == metric].set_index("Model").loc[model_order].reset_index()

    # Scatter
    ax.scatter(df_sub["Mean"], df_sub["Model"], s=80, color="black", zorder=3)

    # Error bars
    ax.errorbar(df_sub["Mean"], df_sub["Model"],
                xerr=df_sub["SD"], fmt='o', color="black",
                capsize=3, markersize=6, lw=1, zorder=2)

    ax.grid(False)
    ax.set_title(metric, fontsize=14, fontweight="bold")
    ax.set_xlim(0.20, 0.85)
    ax.set_xlabel("Score (mean ¬± SD)", fontsize=12, fontweight="bold")
    ax.set_ylabel("")

    # Black border
    for spine in ax.spines.values():
        spine.set_visible(True)
        spine.set_linewidth(1.3)
        spine.set_edgecolor("black")

# Remove unused panels
for j in range(len(metric_order), 9):
    axes[j].axis("off")

plt.tight_layout()
plt.savefig("ML_Genes_3x3_Benchmark.png", dpi=600, facecolor="white", bbox_inches="tight")

print("‚úî Saved: ML_Genes_3x3_Benchmark.png")

# ============================================================
# STEP 11 ‚Äî RADAR PLOT FOR MODELS
# ============================================================

print("\n===== GENERATING RADAR PLOT =====")

radar = df_results.pivot(index="Model", columns="Metric", values="Mean")
radar = radar.loc[model_order, ["accuracy","balanced_accuracy",
                                "f1_macro","kappa","mcc","macro_mcc"]]

# Normalize each metric (0‚Äì1 scaling)
radar_norm = (radar - radar.min()) / (radar.max() - radar.min())

labels = radar_norm.columns
num_vars = len(labels)

angles = np.linspace(0, 2*np.pi, num_vars, endpoint=False).tolist()
angles += angles[:1]

plt.figure(figsize=(6,5), dpi=600)

for model in radar_norm.index:
    values = radar_norm.loc[model].tolist()
    values += values[:1]
    plt.polar(angles, values, marker="o", linewidth=1.5, label=model)

plt.xticks(angles[:-1], labels, fontsize=7, weight="bold")
plt.yticks(fontsize=6)

plt.legend(loc="upper right", bbox_to_anchor=(1.7, 1.1), fontsize=7.5)

plt.savefig("ML_Genes_RadarPlot.png", dpi=600, facecolor="white", bbox_inches="tight")
plt.show()

print("‚úî Saved: ML_Genes_RadarPlot.png")
print("üéâ Completed ML benchmarking + visualization for GENES.\n")

# ============================================================
# NEW 3√ó3 HIGH-PUBLICATION-QUALITY BENCHMARK FIGURE
# ============================================================

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

print("\n===== GENERATING IMPROVED 3√ó3 BENCHMARK FIGURE =====")

df_plot = df_results.copy()

# Clean metric names
df_plot["Metric"] = (
    df_plot["Metric"]
    .str.replace("_", " ")
    .str.title()
    .replace({"Mcc": "MCC", "Macro Mcc": "Macro-MCC"})
)

metric_order = [
    "Accuracy",
    "Balanced Accuracy",
    "F1 Macro",
    "Kappa",
    "MCC",
    "Macro-MCC"
]

model_order = [
    "DecisionTree",
    "RandomForest",
    "LogisticRegression",
    "SVM",
    "GradientBoosting"
]

# Publication color palette (matches your nice figure)
model_colors = {
    "DecisionTree": "#1f77b4",       # blue
    "RandomForest": "#ff7f0e",       # orange
    "LogisticRegression": "#2ca02c", # green
    "SVM": "#d62728",                # red
    "GradientBoosting": "#9467bd"    # purple
}

# Custom axis limits PER metric (fixes MCC/Kappa compression)
axis_ranges = {
    "Accuracy": (0.25, 0.75),
    "Balanced Accuracy": (0.25, 0.75),
    "F1 Macro": (0.25, 0.75),
    "Kappa": (0.10, 0.45),
    "MCC": (0.10, 0.45),
    "Macro-MCC": (0.10, 0.45)
}

# ----------------------------------------
# Create a 3√ó3 layout (only 6 panels used)
# ----------------------------------------
fig, axes = plt.subplots(3, 3, figsize=(18, 14), dpi=600)
axes = axes.flatten()

sns.set_theme(style="white")

for i, metric in enumerate(metric_order):

    ax = axes[i]

    sub = df_plot[df_plot["Metric"] == metric].copy()
    sub = sub.set_index("Model").loc[model_order].reset_index()

    means = sub["Mean"].values
    sds   = sub["SD"].values
    labels = sub["Model"].values

    ypos = np.arange(len(labels))
    colors = [model_colors[m] for m in labels]

    # Black error bar
    ax.errorbar(
        means, ypos,
        xerr=sds,
        fmt='o',
        markersize=8,
        lw=1.5,
        capsize=3,
        color="black",
        markerfacecolor="black",
        zorder=3
    )

    # Colored dots
    ax.scatter(
        means, ypos,
        s=120,
        c=colors,
        edgecolors="black",
        linewidths=1.1,
        zorder=5
    )

    # Y labels
    ax.set_yticks(ypos)
    ax.set_yticklabels(labels, fontsize=10, fontweight="bold")

    # üëâ FIXED axis ranges per metric
    ax.set_xlim(axis_ranges[metric])

    ax.set_xlabel("Score (mean ¬± SD)", fontsize=12, weight="bold")
    ax.set_title(metric, fontsize=15, weight="bold", pad=4)

    ax.grid(False)

    # Black frame
    for spine in ax.spines.values():
        spine.set_visible(True)
        spine.set_linewidth(1.3)
        spine.set_edgecolor("black")

# Turn off remaining unused subplots
for j in range(len(metric_order), 9):
    axes[j].axis("off")

plt.tight_layout()
plt.savefig("ML_Genes_3x3_Publication.png", dpi=600, bbox_inches="tight")
plt.show()

print("‚úî Saved ML_Genes_3x3_Publication.png")

# ============================================================
# Nested Cross-Validation + Holdout (RandomForest & GradientBoosting)
# ============================================================

import numpy as np
import pandas as pd
from collections import defaultdict
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, f1_score,
    cohen_kappa_score, matthews_corrcoef
)
from sklearn.preprocessing import StandardScaler, LabelBinarizer
from sklearn.pipeline import Pipeline

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize
import joblib
import os

# ============================================================
# MACRO-MCC FUNCTION
# ============================================================

def macro_mcc(y_true, y_pred):
    lb = LabelBinarizer()
    lb.fit(y_true)
    Y_true = lb.transform(y_true)
    Y_pred = lb.transform(y_pred)
    return np.mean([matthews_corrcoef(Y_true[:, i], Y_pred[:, i])
                    for i in range(Y_true.shape[1])])


# ============================================================
# 1Ô∏è‚É£ Load Gene TPM + Metadata
# ============================================================

tpm = pd.read_csv("ARG_VF_TPM_clr_batch_corrected_v2.csv", index_col=0)
meta = pd.read_csv("ARG_VF_metadata_final_matched.csv")

tpm.index = tpm.index.astype(str).str.strip()
meta["Sample_ID"] = meta["Sample_ID"].astype(str).str.strip()

# Align samples
common = tpm.index.intersection(meta["Sample_ID"])
assert len(common) > 0, "‚ùå No overlapping sample IDs!"

tpm = tpm.loc[common]
meta = meta.set_index("Sample_ID").loc[common]

X = tpm.values
y = meta["Group"].values

print("‚úî Data aligned ‚Äî genes:", X.shape, "labels:", np.unique(y))


# ============================================================
# 2Ô∏è‚É£ Train / Test Split
# ============================================================

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Train size: {len(y_train)} | Test size: {len(y_test)}")


# ============================================================
# 3Ô∏è‚É£ MODELS ‚Äî RandomForest & GradientBoosting
# ============================================================

models = {
    "RandomForest": Pipeline([
        ("clf", RandomForestClassifier(
            class_weight="balanced",
            random_state=42, n_jobs=-1
        ))
    ]),

    "GradientBoosting": Pipeline([
        ("clf", GradientBoostingClassifier(
            random_state=42
        ))
    ])
}

# Optimized but not excessive grid
param_grids = {
    "RandomForest": {
        "clf__n_estimators": [300, 600],
        "clf__max_depth": [10, 20, None],
        "clf__min_samples_split": [2, 5],
        "clf__min_samples_leaf": [1, 2],
        "clf__max_features": ["sqrt"],
    },

    "GradientBoosting": {
        "clf__n_estimators": [200, 400],
        "clf__learning_rate": [0.01, 0.05, 0.1],
        "clf__max_depth": [2, 3],
        "clf__subsample": [0.8, 1.0],
    }
}


# ============================================================
# 4Ô∏è‚É£ Nested CV Setup
# ============================================================

outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

cv_summary = []
fold_details = []
best_params = {}

metric_list = ["Accuracy","BalancedAcc","F1_macro","Kappa","MCC","Macro_MCC"]


# ============================================================
# 5Ô∏è‚É£ Nested CV Loop
# ============================================================

for name, model in models.items():
    print(f"\nüöÄ Running nested CV for {name}")

    outer_metrics = defaultdict(list)

    for fold, (tr, te) in enumerate(outer.split(X_train, y_train), 1):
        X_tr, X_te = X_train[tr], X_train[te]
        y_tr, y_te = y_train[tr], y_train[te]

        gs = GridSearchCV(
            model, param_grids[name],
            cv=inner, scoring="balanced_accuracy",
            n_jobs=-1
        )
        gs.fit(X_tr, y_tr)
        best = gs.best_estimator_

        y_pred = best.predict(X_te)

        # Store metrics
        outer_metrics["Accuracy"].append(accuracy_score(y_te, y_pred))
        outer_metrics["BalancedAcc"].append(balanced_accuracy_score(y_te, y_pred))
        outer_metrics["F1_macro"].append(f1_score(y_te, y_pred, average="macro"))
        outer_metrics["Kappa"].append(cohen_kappa_score(y_te, y_pred))
        outer_metrics["MCC"].append(matthews_corrcoef(y_te, y_pred))
        outer_metrics["Macro_MCC"].append(macro_mcc(y_te, y_pred))

        fold_details.append(pd.DataFrame({
            "Model": name,
            "Fold": fold,
            **{metric: outer_metrics[metric][-1] for metric in metric_list}
        }, index=[0]))

    # Summary
    cv_summary.append({
        "Model": name,
        **{f"Mean_{m}": np.mean(v) for m, v in outer_metrics.items()},
        **{f"SD_{m}": np.std(v) for m, v in outer_metrics.items()}
    })

    # Fit entire training set to extract final ‚Äúbest parameters‚Äù
    gs_full = GridSearchCV(
        model, param_grids[name],
        cv=inner, scoring="balanced_accuracy", n_jobs=-1
    )
    gs_full.fit(X_train, y_train)
    best_params[name] = gs_full.best_params_

# Save outputs
pd.DataFrame(cv_summary).to_csv("NestedCV_RF_GB_summary.csv", index=False)
pd.concat(fold_details).to_csv("NestedCV_RF_GB_folds.csv", index=False)
pd.DataFrame(best_params).to_csv("NestedCV_RF_GB_best_params.csv")

print("\nüìä Nested CV finished!")
print(best_params)


# ============================================================
# 6Ô∏è‚É£ Hold-out Evaluation
# ============================================================

def evaluate_and_save(model_name, model, X_train, y_train, X_test, y_test, labels):
    print(f"\nüîé Evaluating {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    metrics = {
        "Model": model_name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "BalancedAcc": balanced_accuracy_score(y_test, y_pred),
        "F1_macro": f1_score(y_test, y_pred, average="macro"),
        "Kappa": cohen_kappa_score(y_test, y_pred),
        "MCC": matthews_corrcoef(y_test, y_pred),
        "Macro_MCC": macro_mcc(y_test, y_pred)
    }

    pd.DataFrame([metrics]).to_csv(f"Holdout_{model_name}.csv", index=False)

    return metrics


labels = np.unique(y)

# Build tuned models
rf_best = RandomForestClassifier(
    class_weight="balanced", random_state=42, n_jobs=-1,
    **{k.replace("clf__", ""): v for k,v in best_params["RandomForest"].items()}
)

gb_best = GradientBoostingClassifier(
    random_state=42,
    **{k.replace("clf__", ""): v for k,v in best_params["GradientBoosting"].items()}
)

rf_results = evaluate_and_save("RF", rf_best, X_train, y_train, X_test, y_test, labels)
gb_results = evaluate_and_save("GB", gb_best, X_train, y_train, X_test, y_test, labels)

pd.DataFrame([rf_results, gb_results]).to_csv("Holdout_Summary_RF_GB.csv", index=False)

print("\nüéâ Completed nested CV + holdout for RandomForest & GradientBoosting.")

# ============================================================
# 8Ô∏è‚É£ SAVE FINAL MODELS + PREDICTIONS (RF & GB)
# ============================================================

import joblib
import os

os.makedirs("saved_models", exist_ok=True)
os.makedirs("saved_predictions", exist_ok=True)

# Save test data (for reviewers)
np.save("saved_predictions/X_test.npy", X_test)
np.save("saved_predictions/y_test.npy", y_test)

# ============================================================
# ---- RandomForest ----
# ============================================================

print("\nüíæ Saving RandomForest model and predictions...")

rf_best.fit(X_train, y_train)

# Save model
joblib.dump(rf_best, "saved_models/RandomForest_model.pkl")

# Predictions
rf_pred = rf_best.predict(X_test)
rf_proba = rf_best.predict_proba(X_test)

pd.DataFrame({
    "TrueLabel": y_test,
    "PredLabel": rf_pred
}).to_csv("saved_predictions/RandomForest_predictions.csv", index=False)

pd.DataFrame(
    rf_proba,
    columns=[f"{cls}_prob" for cls in labels]
).to_csv("saved_predictions/RandomForest_proba.csv", index=False)

print("‚úî RandomForest model + predictions saved.")


# ============================================================
# ---- GradientBoosting ----
# ============================================================

print("\nüíæ Saving GradientBoosting model and predictions...")

gb_best.fit(X_train, y_train)

# Save model
joblib.dump(gb_best, "saved_models/GradientBoosting_model.pkl")

# Predictions
gb_pred = gb_best.predict(X_test)
gb_proba = gb_best.predict_proba(X_test)

pd.DataFrame({
    "TrueLabel": y_test,
    "PredLabel": gb_pred
}).to_csv("saved_predictions/GradientBoosting_predictions.csv", index=False)

pd.DataFrame(
    gb_proba,
    columns=[f"{cls}_prob" for cls in labels]
).to_csv("saved_predictions/GradientBoosting_proba.csv", index=False)

print("‚úî GradientBoosting model + predictions saved.")

print("\nüéâ All RF & GB models + predictions saved successfully!\n")